{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PREDICTING FLIGHT DELAYS AT ARRIVAL"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Powered by the <span style=\"color:rgb(0,94,184)\">Data Science Platform and Romain*</span> <img src=\"images/logo-dsp-100x100.png\" style=\"vertical-align:middle\" width=\"25\" height=\"25\" />"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Level: <span style=\"color:green\">Beginner</span>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duration: *40 min*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to to build, train and evaluate a Random Forest model in order to predict the flight delays at arrival of Southwest airline company (WN) at the Los Angeles international airport (LAX) in 2015. The purpose is not to obtain the best possible prediction but rather to emphasize on the various steps needed to build such a model.\n",
        "\n",
        "The main goal of this notebook is to undestand all the principles of machine learning using a tool called Azure Machine Learning. We will go through every steps together to discover all the possibilities of Azure ML.\n",
        "\n",
        "This notebook will cover the features of the Data science platform below:\n",
        "* PySpark notebook\n",
        "* Data access in MaprFS Raw Data Archive using spark\n",
        "* Data preparation using SQL and spark-sql\n",
        "* Data visualization\n",
        "* Build a Random Forest model using spark-ml\n",
        "* Train this random forest model using spark-ml\n",
        "* Evaluate the model using spark-ml\n",
        "* Using Azure ML\n",
        "* Creating a component\n",
        "* Creating an environment with Dockerfile\n",
        "* Creating a Pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/global_free_trial_spark_delay.png\" width=\"700\" height=\"350\" />"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, we have to get a handle to our workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install azure-ai-ml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import os\n",
        "from azure.ai.ml import MLClient\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Execute the script\n",
        "%run setenv.py\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id= os.environ['subscription_id'],\n",
        "    resource_group_name= os.environ['resource_group'],\n",
        "    workspace_name= os.environ['workspace_name']\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1669884929005
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Checking Credentials\n",
        "\n",
        "ml_client is lazy. So your credentials might be invalid. Run this cell to make sure your credentials are correct :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if credentials are valid\n",
        "from IPython.display import Image\n",
        "\n",
        "try :\n",
        "    ml_client.begin_create_or_update(ml_client.workspaces.get())\n",
        "    print(Fore.GREEN + \"Credentials are valid\")\n",
        "except :\n",
        "    print(\"Credentials are invalid\")\n",
        "    print(\"Please check your credentials : subscription_id, resource_group_name, workspace_name must be correct\")\n",
        "    display(Image(filename='images/credentials.PNG'))\n",
        "    print(\"You can find your credentials by clicking on the TOP LEFT of the Azure Portal ML Studio\")\n",
        "    display(Image(filename='images/values.png'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Credentials are valid\n"
        }
      ],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a compute resource to run your job\n",
        "\n",
        "You'll need a compute resource for running a job. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n",
        "\n",
        "You'll provision a Linux compute cluster. See the [full list on VM sizes and prices](https://azure.microsoft.com/pricing/details/machine-learning/) .\n",
        "\n",
        "For this example, you only need a basic cluster, so you'll use a Standard_DS11_v2 model and create an Azure ML Compute."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster-flights\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_DS11_V2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=4,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    \n",
        "\n",
        "print(\n",
        "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named cpu-cluster-flights, we'll reuse it as is.\nAMLCompute with name cpu-cluster-flights is created, the compute size is STANDARD_DS11_V2\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1669884933949
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Create Environnment for our execution/job"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run your AzureML job on your compute resource, you'll need an environment. An environment lists the software runtime and libraries that you want installed on the compute where you’ll be training. It's similar to your python environment on your local machine.\n",
        "\n",
        "AzureML provides many curated or ready-made environments, which are useful for common training and inference scenarios. You can also create your own custom environments using a docker image, or a conda configuration.\n",
        "\n",
        "In this example, you'll create a custom conda environment for your jobs, using a conda yaml file.\n",
        "\n",
        "First, create a directory to store the file in."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "env_dir = \"./env\"\n",
        "os.makedirs(env_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1669799861825
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create the Dockerfile where our pipeline will run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {env_dir}/Dockerfile\n",
        "FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20220902.v1\n",
        "\n",
        "ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/responsibleai-0.21\n",
        "\n",
        "# Install wkhtmltopdf for pdf rendering from html\n",
        "RUN apt-get -y update && apt-get -y install wkhtmltopdf\n",
        "\n",
        "# Create conda environment\n",
        "RUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n",
        "    python=3.8 pip=21.3.1 -c anaconda -c conda-forge\n",
        "\n",
        "# Prepend path to AzureML conda environment\n",
        "ENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\n",
        "\n",
        "# Install pip dependencies\n",
        "# markupsafe and itsdangerous are bug workarounds\n",
        "# install azureml-defaults==1.47.0\n",
        "    # inference-schema[numpy-support]==1.5\n",
        "    # joblib==1.0.1\n",
        "RUN pip install 'responsibleai~=0.21.0' \\\n",
        "                'raiwidgets~=0.21.0' \\\n",
        "                'pyarrow' \\\n",
        "                'markupsafe<=2.0.1' \\\n",
        "                'itsdangerous==2.0.1' \\\n",
        "                'mlflow==1.30.0' \\\n",
        "                'scikit-learn<1.1' \\\n",
        "                'pdfkit==1.0.0' \\\n",
        "                'plotly==5.6.0' \\\n",
        "                'kaleido==0.2.1' \\\n",
        "                'azureml-core==1.47.0' \\\n",
        "                'azureml-dataset-runtime==1.47.0' \\\n",
        "                'azureml-mlflow==1.47.0' \\\n",
        "                'azureml-telemetry==1.47.0'\\\n",
        "                'seaborn'\\\n",
        "                'matplotlib'\\\n",
        "                'pyspark>=3.1,<3.2'\\\n",
        "                'azureml-defaults==1.47.0'\\\n",
        "                'inference-schema[numpy-support]==1.5'\\\n",
        "                'joblib==1.0.1'\n",
        "                \n",
        "                    \n",
        "\n",
        "RUN pip install --pre 'azure-ai-ml'\n",
        "\n",
        "# no-deps install for domonic due to unresolvable dependencies requirment on urllib3 and requests.\n",
        "# score card rendering is using domonic only for the html elements composer which does not involve requests or urllib3\n",
        "RUN pip install --no-deps 'charset-normalizer==2.0.12' \\\n",
        "                          'cssselect==1.1.0' \\\n",
        "                          'elementpath==2.5.0' \\\n",
        "                          'html5lib==1.1' \\\n",
        "                          'webencodings==0.5.1' \\\n",
        "                          'domonic==0.9.10'\n",
        "\n",
        "# This is needed for mpi to locate libpython\n",
        "ENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH/lib:$LD_LIBRARY_PATH\n",
        "\n",
        "# This is needed for pyspark to locate Java\n",
        "RUN apt-get update && \\\n",
        "    mkdir -p /usr/share/man/man1 && \\\n",
        "    apt-get install -y openjdk-8-jdk && \\\n",
        "    apt-get install -y ant && \\\n",
        "    apt-get clean && \\\n",
        "    rm -rf /var/lib/apt/lists/ && \\\n",
        "    rm -rf /var/cache/oracle-jdk8-installer;\n",
        "    \n",
        "ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\n",
        "RUN export JAVA_HOME\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./env/Dockerfile\n"
        }
      ],
      "execution_count": 46,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use a job to register our environment into our workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "from azure.ai.ml.entities import BuildContext\n",
        "import os\n",
        "\n",
        "custom_env_name = \"flight-delays-custom-env\"\n",
        "\n",
        "buildcontext = BuildContext(\n",
        "    path=env_dir\n",
        ")\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for spark flight delays\",\n",
        "    tags={\"owner\": os.environ[\"owner\"], \"created\": \"2022-11-23\"},\n",
        "    build=buildcontext,\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading env (0.0 MBs): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2799/2799 [00:00<00:00, 67010.99it/s]\u001b[0m\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name flight-delays-custom-env is registered to workspace, the environment version is 12\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1669799887523
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Writing our component\n",
        "First of all, we are creating a yml file. This file will be a description of our azure ml component. It explains how this component works."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "component_dir = \"./components\"\n",
        "os.makedirs(component_dir, exist_ok=True)\n",
        "\n",
        "analysis_src_dir = \"./components/src\"\n",
        "os.makedirs(analysis_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1669800336891
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $component_dir/flight_delays_analysis.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: analysis\n",
        "display_name: analysis\n",
        "version: 29\n",
        "type: command\n",
        "inputs:\n",
        "  public_data_flight_delays: \n",
        "    type: uri_file\n",
        "outputs:\n",
        "  eval_output:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:flight-delays-custom-env@latest\n",
        "command: >-\n",
        "  python analysis.py\n",
        "  --public_data_flight_delays ${{inputs.public_data_flight_delays}} \n",
        "  --eval_output ${{outputs.eval_output}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/flight_delays_analysis.yml\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1669192288076
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to actually create the code that will be executed by our component"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {analysis_src_dir}/analysis.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import datetime, warnings, scipy \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "from collections import OrderedDict\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pathlib import Path\n",
        "\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--public_data_flight_delays\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--eval_output\", type=str, help=\"path to eval output\")\n",
        "    args = parser.parse_args()\n",
        "   \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    ###################\n",
        "    #<load the data>\n",
        "    ###################\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.public_data_flight_delays)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    flights = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays, \"flights.csv\"), header=True)\n",
        "    airlines_names = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays,\"airlines.csv\"), header=True).toPandas()\n",
        "    \n",
        "    ####################\n",
        "    #</load the data>\n",
        "    ####################\n",
        "\n",
        "    ####################\n",
        "    #<Metrics rows and columns>\n",
        "    ####################\n",
        "    mlflow.log_metric(\"nb_rows\", flights.count())\n",
        "    mlflow.log_metric(\"nb_columns\", len(flights.columns))\n",
        "\n",
        "    ####################\n",
        "    #</Metrics rows and columns>\n",
        "    ####################\n",
        "\n",
        "    ####################\n",
        "    #<sample csv load>\n",
        "    ####################\n",
        "    tab_info=pd.DataFrame(flights.dtypes).T.rename(index={0:'column type'})\n",
        "\n",
        "    sample = flights.limit(10).toPandas()\n",
        "\n",
        "    sample.to_csv(\"sample.csv\")\n",
        "\n",
        "    mlflow.log_artifact(\"sample.csv\")\n",
        "\n",
        "    flights.createOrReplaceTempView(\"flights\")\n",
        "\n",
        "    airline_delays = spark.sql( \\\n",
        "        \"SELECT AIRLINE, MIN(ARRIVAL_DELAY) as min, MAX(ARRIVAL_DELAY) as max, AVG(ARRIVAL_DELAY) as avg, count(*) as count \\\n",
        "        from flights WHERE MONTH == 1 AND ARRIVAL_DELAY is not null \\\n",
        "        group by AIRLINE order by count\")\n",
        "\n",
        "    tab_pandas = airline_delays.toPandas()\n",
        "    \n",
        "    tab_pandas.to_csv(\"airline_delays.csv\")\n",
        "\n",
        "    mlflow.log_artifact(\"airline_delays.csv\")\n",
        "\n",
        "    ####################\n",
        "    #</sample csv load>\n",
        "    ####################\n",
        "\n",
        "\n",
        "    ####################\n",
        "    #<Graph Visualisation>\n",
        "    ####################\n",
        "    # 1.6 Flight & Mean Flight Delay distribution per Airline\n",
        "\n",
        "    abbr_companies = airlines_names.set_index('IATA_CODE')['AIRLINE'].to_dict()\n",
        "    global_stats = airline_delays.toPandas()\n",
        "    font = {'family' : 'normal', 'weight' : 'bold', 'size'   : 15}\n",
        "    mpl.rc('font', **font)\n",
        "    #__________________________________________________________________\n",
        "    colors = ['royalblue', 'grey', 'wheat', 'c', 'firebrick', 'seagreen', 'lightskyblue',\n",
        "            'lightcoral', 'yellowgreen', 'gold', 'tomato', 'violet', 'aquamarine', 'chartreuse']\n",
        "    #___________________________________\n",
        "    fig = plt.figure(1, figsize=(16,15))\n",
        "    gs=GridSpec(2,2)             \n",
        "    ax1=fig.add_subplot(gs[0,0]) \n",
        "    ax2=fig.add_subplot(gs[0,1])  \n",
        "    #------------------------------\n",
        "    # Pie chart nº1: nb of flights\n",
        "    #------------------------------\n",
        "    labels = [s for s in  global_stats.index]\n",
        "    sizes  = global_stats['count'].values\n",
        "    explode = [0.3 if sizes[i] < 20000 else 0.0 for i in range(len(abbr_companies))]\n",
        "    patches, texts, autotexts = ax1.pie(sizes, explode = explode,\n",
        "                                    labels=labels, colors = colors,  autopct='%1.0f%%',\n",
        "                                    shadow=False, startangle=0)\n",
        "    for i in range(len(abbr_companies)): \n",
        "        texts[i].set_fontsize(14)\n",
        "    ax1.axis('equal')\n",
        "    ax1.set_title('% of flights per company', bbox={'facecolor':'midnightblue', 'pad':5},\n",
        "                color = 'w',fontsize=18)\n",
        "    #_______________________________________________\n",
        "    # I set the legend: abreviation -> airline name\n",
        "    comp_handler = []\n",
        "    i = 0\n",
        "    for company in abbr_companies:\n",
        "        comp_handler.append(mpatches.Patch(color=colors[i],\n",
        "                label = abbr_companies[company]))\n",
        "        i = i + 1\n",
        "    ax1.legend(handles=comp_handler, bbox_to_anchor=(0.2, 0.9), \n",
        "            fontsize = 13, bbox_transform=plt.gcf().transFigure)\n",
        "    #----------------------------------------\n",
        "    # Pie chart nº2: mean delay at arrival\n",
        "    #----------------------------------------\n",
        "    sizes  = global_stats['avg'].values\n",
        "    sizes  = [max(s,0) for s in sizes]\n",
        "    explode = [0.0 if sizes[i] < 20000 else 0.01 for i in range(len(abbr_companies))]\n",
        "    patches, texts, autotexts = ax2.pie(sizes, explode = explode, labels = labels,\n",
        "                                    colors = colors, shadow=False, startangle=0,\n",
        "                                    autopct = lambda p :  '{:.0f}'.format(p * sum(sizes) / 100))\n",
        "    for i in range(len(abbr_companies)): \n",
        "        texts[i].set_fontsize(14)\n",
        "    ax2.axis('equal')\n",
        "    ax2.set_title('Mean delay at arrival', bbox={'facecolor':'midnightblue', 'pad':5},\n",
        "                color='w', fontsize=18)\n",
        "    #________________________\n",
        "    plt.tight_layout(w_pad=3)\n",
        "    plt.subplots()\n",
        "    #________________________\n",
        "    mlflow.log_figure(fig, \"flight_delay_per_airline.png\")\n",
        "\n",
        "    ####################\n",
        "    #</Graph Visualisation>\n",
        "    ####################\n",
        "\n",
        "    # 1.7. How airport impacts delays - Mean delays at arrival for each airport for Southwest airline\n",
        "\n",
        "    # Reset plot\n",
        "    plt.clf()\n",
        "\n",
        "    airline = \"'WN'\" # South West\n",
        "\n",
        "    airports_delays = spark.sql( \\\n",
        "    \"SELECT DESTINATION_AIRPORT, MIN(ARRIVAL_DELAY) as min, MAX(ARRIVAL_DELAY) as max, AVG(ARRIVAL_DELAY) as avg, count(*) as count \\\n",
        "    from flights WHERE MONTH == 1 AND AIRLINE == \" + airline + \" AND ARRIVAL_DELAY is not null \\\n",
        "    group by DESTINATION_AIRPORT order by DESTINATION_AIRPORT\")\n",
        "\n",
        "    airports_delays.limit(10).toPandas().to_csv(\"airports_delays.csv\")\n",
        "    mlflow.log_artifact(\"airports_delays.csv\")\n",
        "    \n",
        "\n",
        "    # Output of our component\n",
        "    eval_msg = f\"Eval done\\n\"\n",
        "    (Path(args.eval_output) / \"eval_result.txt\").write_text(eval_msg)\n",
        "   \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./components/src/analysis.py\n"
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Creating our component with a command\n",
        "Open a terminal\n",
        "\n",
        "<img src=\"images/openTerminal.PNG\" width=\"800\" height=\"600\" />\n",
        "\n",
        "Go into the right folder\n",
        "```\n",
        "$ cd flightdelaytuto/tutoAzureML/\n",
        "```\n",
        "Execute the setenv.py script\n",
        "\n",
        "```\n",
        "$ run setenv.py\n",
        "```\n",
        "\n",
        "```\n",
        "$ cd components\n",
        "```\n",
        "\n",
        "```\n",
        "$ az login --tenant $tenant_id\n",
        "```\n",
        "\n",
        "```\n",
        "$ az configure --defaults workspace=<Azure Machine Learning workspace name> group=<resource group>\n",
        "```\n",
        "\n",
        "\n",
        "After you successfuly login, set the right subscription ID you are currently using in Microsoft Azure Machine learning Studio.\n",
        "\n",
        "\n",
        "**az account set --subscription 8b5374c5-8b98-45fb-bd96-7d5a4ce4e527**\n",
        "\n",
        "You are now perfectly set up and can create a component with this command using your workspace-name and ressource-group :\n",
        "\n",
        "**az ml component create --file flight_delays_analysis.yml --workspace-name romain-mlbox --resource-group rg-sbx-aiops**\n",
        "\n",
        "It should display a JSON with informations of the component you just created"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Creating a pipeline with the designer tool\n",
        "\n",
        "Open a new window of Azure ML Studio\n",
        "\n",
        "Go to Pipelines and create a new pipeline drafts\n",
        "\n",
        "Select Custom using custom components\n",
        "\n",
        "You should now see the designer tool\n",
        "\n",
        "Now select in Data, the dataset-delays-flights and drag it into the pipeline model\n",
        "\n",
        "Select Component and drop analysis component into the pipeline model\n",
        "\n",
        "Now drag the Data output port into public_flight_delays input port\n",
        "\n",
        "Rename the pipeline with the name : **Analysis Delay Flights**\n",
        "\n",
        "Go to settings and Select a Compute Cluster type and select **cpu-cluster-flights** that we create earlier\n",
        "\n",
        "Your screen should look like this :\n",
        "\n",
        "<img src=\"images/designer_tool.PNG\" width=\"800\" height=\"390\" />\n",
        "\n",
        "It is time to press Submit to launch this pipeline for the first time\n",
        "\n",
        "**Note that a .yml file can completely replace the design tool**\n",
        "\n",
        "In Assets Jobs on the left you should see your job\n",
        "\n",
        "Pipeline job overview is where you should see your pipeline running. It can take 5 to 15 minutes to run (installing environment on a node...)\n",
        "\n",
        "**Important**\n",
        "Whenever the job is complete, you can see a lot of informations in the component analysis that has finished :\n",
        "\n",
        "-Output + Logs = when you see the csv that has been generate by the pipeline and the output file\n",
        "\n",
        "-Metrics : Values that have been registered such as nb of rows\n",
        "* 5,819,079 rows\n",
        "* 31 columns\n",
        "\n",
        "-Images : Images that have been loaded during the execution"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Model training: Single airport LAX, Single airline WN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previsous sections dealt with an exploration of the dataset.  \n",
        "From here, we start with the modeling of flight delays. \n",
        "\n",
        "**Motivations**\n",
        "\n",
        "There is a high variability in average delays, both between the different airports but also between the different airlines. So, it is necessary to learn model that is specific to an airline the destination airport.\n",
        "\n",
        "We will learn model that predicts the flight delays at the destination airport at a given time of arrival (on January). We will work on Southwest Airlines (WN) flights arriving at the airport of Los Angeles International Airport (LAX). We will use the 3 first weeks of January as the training set and the follwoing week of January as the test set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Training Component\n",
        "\n",
        "Now we create our second component which will create a model and train the model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $component_dir/flight_delays_training.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: train_delay_model\n",
        "display_name: train_delay_model\n",
        "version: 11\n",
        "type: command\n",
        "inputs:\n",
        "  public_data_flight_delays: \n",
        "    type: uri_file\n",
        "  num_trees:\n",
        "    type: integer\n",
        "    default: 100\n",
        "outputs:\n",
        "  eval_output:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:flight-delays-custom-env@latest\n",
        "command: >-\n",
        "  python train.py\n",
        "  --public_data_flight_delays ${{inputs.public_data_flight_delays}} \n",
        "  --num_trees ${{inputs.num_trees}}\n",
        "  --eval_output ${{outputs.eval_output}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/flight_delays_training.yml\n"
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the code of our component"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {analysis_src_dir}/train.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import datetime, warnings, scipy \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "from collections import OrderedDict\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pathlib import Path\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "\n",
        "def format_date(year, month, day, scheduled):\n",
        "    if scheduled == 2400: \n",
        "        scheduled = 0\n",
        "    scheduled = \"{0:04d}\".format(int(scheduled))\n",
        "    return datetime.datetime(year, month, day, int(scheduled[0:2]), int(scheduled[2:4])).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def format_hour(scheduled):\n",
        "    if scheduled == 2400: \n",
        "        scheduled = 0\n",
        "    scheduled = \"{0:04d}\".format(int(scheduled))\n",
        "    return int(scheduled[0:2])\n",
        "    \n",
        "def format_seconds(scheduled):\n",
        "    if scheduled == 2400: \n",
        "        scheduled = 0\n",
        "    scheduled = \"{0:04d}\".format(int(scheduled))\n",
        "    return (3600 * int(scheduled[0:2])) + (60 * int(scheduled[2:4]))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--public_data_flight_delays\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--num_trees\", type=int, required=False, default=100)\n",
        "    parser.add_argument(\"--eval_output\", type=str, help=\"path to eval output\")\n",
        "    args = parser.parse_args()\n",
        "   \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    ###################\n",
        "    #<load the data>\n",
        "    ###################\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.public_data_flight_delays)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    flights = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays, \"flights.csv\"), header=True)\n",
        "    airlines_names = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays,\"airlines.csv\"), header=True).toPandas()\n",
        "    \n",
        "    ####################\n",
        "    #</load the data>\n",
        "    ####################\n",
        "    udf_format_date = F.udf(format_date, StringType())\n",
        "    udf_format_hour = F.udf(format_hour, IntegerType())\n",
        "    udf_format_seconds = F.udf(format_seconds, IntegerType())\n",
        "    flights = flights \\\n",
        "        .withColumn('SCHEDULED_DEPARTURE_FORMATTED',udf_format_date(flights.YEAR, flights.MONTH, flights.DAY, flights.SCHEDULED_DEPARTURE)) \\\n",
        "        .withColumn('SCHEDULED_ARRIVAL_FORMATTED',udf_format_date(flights.YEAR, flights.MONTH, flights.DAY, flights.SCHEDULED_ARRIVAL)) \\\n",
        "        .withColumn('SCHEDULED_ARRIVAL_hour',udf_format_hour(flights.SCHEDULED_ARRIVAL)) \\\n",
        "        .withColumn('SCHEDULED_ARRIVAL_sec',udf_format_seconds(flights.SCHEDULED_ARRIVAL))\n",
        "\n",
        "    flights.limit(10).toPandas().to_csv(\"flights.csv\")\n",
        "    mlflow.log_artifact(\"flights.csv\")\n",
        "\n",
        "    airline = \"'WN'\"\n",
        "    airport = \"'LAX'\"\n",
        "\n",
        "    flights.createOrReplaceTempView(\"flights\")\n",
        "\n",
        "    df_fi = spark.sql( \\\n",
        "        \"SELECT DAY_OF_WEEK, SCHEDULED_ARRIVAL_sec, DAY, ARRIVAL_DELAY as label \\\n",
        "        from flights WHERE AIRLINE == \" + airline + \" AND DESTINATION_AIRPORT == \" + airport +  \" AND ARRIVAL_DELAY is not null AND MONTH == 1\")\n",
        "\n",
        "    df_fi.limit(10).toPandas().to_csv(\"df_fi.csv\")\n",
        "    mlflow.log_artifact(\"df_fi.csv\")\n",
        "\n",
        "    df_train = df_fi.filter(F.col('DAY') < 23).drop('DAY')\n",
        "    df_test = df_fi.filter(F.col('DAY') > 23).drop('DAY')\n",
        "    print(\"training dataset size: \" + str(df_train.count()))\n",
        "    print(\"test dataset size: \" + str(df_test.count()))\n",
        "    df_train.limit(10).toPandas().to_csv(\"df_train.csv\")\n",
        "    mlflow.log_artifact(\"df_train.csv\")\n",
        "\n",
        "    assembler = VectorAssembler(\n",
        "    inputCols=[\"DAY_OF_WEEK\", \"SCHEDULED_ARRIVAL_sec\"], outputCol=\"features\"\n",
        "    )\n",
        "\n",
        "    X_train = assembler.transform(df_train)\n",
        "    X_train.limit(10).toPandas().to_csv(\"X_train.csv\")\n",
        "    mlflow.log_artifact(\"X_train.csv\")\n",
        "\n",
        "    # 2.2.1 Build, Train Model and visualize feature importances\n",
        "    # Train a RandomForest model\n",
        "    rf = RandomForestRegressor(numTrees=args.num_trees, featuresCol='features',labelCol='label',predictionCol='prediction')\n",
        "    rfModel = rf.fit(X_train)\n",
        "\n",
        "    importances = rfModel.featureImportances\n",
        "\n",
        "    x_values = list(range(len(importances)))\n",
        "\n",
        "    # Visualize the feature importances\n",
        "    plt.bar(x_values, importances, orientation = 'vertical')\n",
        "    plt.xticks(x_values, [\"DAY_OF_WEEK\", \"SCHEDULED_ARRIVAL_sec\"], rotation=40)\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xlabel('Feature')\n",
        "    plt.title('Feature Importances')\n",
        "\n",
        "    plt.savefig(\"feature_importances.png\")\n",
        "    mlflow.log_artifact(\"feature_importances.png\")\n",
        "\n",
        "    # 2.3 Model evaluation\n",
        "    X_test = assembler.transform(df_test)\n",
        "    predictions = rfModel.transform(X_test)\n",
        "    predictions.select(\"prediction\", \"label\", \"features\").limit(10).toPandas().to_csv(\"predictions.csv\")\n",
        "    mlflow.log_artifact(\"predictions.csv\")\n",
        "\n",
        "    evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "    mse = evaluator.evaluate(predictions)\n",
        "    print(\"Mean Squared Error (MSE) on test data = %g\" % mse)\n",
        "\n",
        "    mlflow.log_metric(\"mse\", mse)\n",
        "\n",
        "    # 2.4 Model deployment\n",
        "    #TODO: add model deployment code\n",
        "\n",
        "    # get the current run id\n",
        "    run_id = mlflow.active_run().info.run_id\n",
        "\n",
        "    # Save the model\n",
        "    mlflow.spark.log_model(rfModel, \"model_delays_flight\")\n",
        "    mlflow.spark.save_model(rfModel, \"model_delays_flight\")\n",
        "    \n",
        "    mlflow.register_model(\"runs:/\" + run_id + \"/model_delays_flight\", \"model_delays_flight\")\n",
        "\n",
        "\n",
        "    # Output of our component\n",
        "    eval_msg = f\"Eval done\\n\"\n",
        "    (Path(args.eval_output) / \"eval_result.txt\").write_text(eval_msg)\n",
        "   \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/src/train.py\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Creating an other component with a command\n",
        "Open a terminal in this folder\n",
        "\n",
        "**cd components**\n",
        "\n",
        "Now we should already been logged. If there is any problem go to the login instructions\n",
        "\n",
        "**az ml component create --file flight_delays_training.yml --workspace-name romain-mlbox --resource-group rg-sbx-aiops**\n",
        "\n",
        "It should display a JSON with informations of the component you just created"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Updating our pipeline with the designer tool\n",
        "\n",
        "Open a new window of Azure ML Studio\n",
        "\n",
        "Go to Pipelines and create a new pipeline drafts\n",
        "\n",
        "Select Custom using custom components\n",
        "\n",
        "You should now see the designer tool\n",
        "\n",
        "Select Component and drop analysis component into the pipeline model\n",
        "\n",
        "Now drag the Data output port into public_flight_delays input port from train_delay_model component\n",
        "\n",
        "Rename the pipeline with the name : **Analysis and Train Delay Flights**\n",
        "\n",
        "Click on the train_delay_model component, you should be able to see a parameters called num_trees which is used by our component when it trains our model with randomForest algorithm\n",
        "\n",
        "\n",
        "It is time to press Submit to launch this pipeline\n",
        "\n",
        "Now you can see that analysis composent finished instantly because all the steps have been reused fron the precedent execution\n",
        "\n",
        "**Note that a .yml file can completely replace the design tool**\n",
        "\n",
        "**Important**\n",
        "Whenever the job is complete, you can see a lot of informations in the component analysis/train_delay_model that has finished :\n",
        "\n",
        "-Output + Logs = when you see the csv that has been generate by the pipeline and the output file\n",
        "\n",
        "-Metrics : Values that have been registered such as nb of rows\n",
        "* 5,819,079 rows\n",
        "* 31 columns\n",
        "\n",
        "-Images : Images that have been loaded during the execution"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create endpoint and deployment\n",
        "\n",
        "Online endpoints are endpoints that are used for online (real-time) inferencing\n",
        "Our goal here is to deploy our model so everyone can use this model to get a prediction.\n",
        "Endpoint is a server that runs into Azure CPU or GPU. You can request a prediction with REST API when it deploys\n",
        "\n",
        "## 2.1 Create endpoint"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a local endpoint\n",
        "import datetime\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        ")\n",
        "\n",
        "online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is an online endpoint\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\"ama_owner\": \"romain.caret@amadeus.com\"},\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1669884951552
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_create_or_update(endpoint)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "<azure.core.polling._poller.LROPoller at 0x7f699c706610>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1669884956013
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "score_dir = \"./model_scoring\"\n",
        "os.makedirs(score_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1669884992423
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "SLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pyspark/jars/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pyspark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n"
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT EXECUTE THIS CELL\n",
        "\n",
        "# The Model Name\n",
        "model_name = \"model_delays_flight\"\n",
        "\n",
        "# get the uri of the model\n",
        "model_uri = \"models:/\" + model_name + \"/latest\"\n",
        "\n",
        "rfModel = mlflow.pyfunc.load_model(model_uri)\n",
        "\n",
        "file = mlflow.pyfunc.get_model_dependencies(model_uri)\n",
        "\n",
        "print(file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import json\n",
        "import numpy\n",
        "import joblib\n",
        "import mlflow\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "\"\"\"\n",
        "This function is called for every invocation of the endpoint to perform the actual scoring/prediction.\n",
        "In the example we extract the data from the json input and call the model's predict()\n",
        "method and return the result back\n",
        "\n",
        ":param raw_data: The input data in json format\n",
        ":return: The prediction result in json format\n",
        "\n",
        "the format of the input data is:\n",
        "\n",
        "{\n",
        "    \"data\": [\n",
        "        {\n",
        "            \"DAY_OF_WEEK\": 1,\n",
        "            \"SCHEDULED_ARRIVAL_sec\": 0\n",
        "        },\n",
        "        {\n",
        "            \"DAY_OF_WEEK\": 2,\n",
        "            \"SCHEDULED_ARRIVAL_sec\": 1\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\"\"\"\n",
        "conf = SparkConf().setAppName(\"appName\").setMaster(\"local\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "data = [\n",
        "        {\n",
        "            \"DAY_OF_WEEK\": 6,\n",
        "            \"SCHEDULED_ARRIVAL_sec\": 27000\n",
        "        },\n",
        "        {\n",
        "            \"DAY_OF_WEEK\": 6,\n",
        "            \"SCHEDULED_ARRIVAL_sec\": 33900\n",
        "        }\n",
        "    ]\n",
        "\n",
        "# Transform the data into a Spark DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Create a VectorAssembler to transform the data into a vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"DAY_OF_WEEK\", \"SCHEDULED_ARRIVAL_sec\"],\n",
        "    outputCol=\"features\")\n",
        "\n",
        "# Transform the data\n",
        "df = assembler.transform(df)\n",
        "\n",
        "\n",
        "model_name = \"model_delays_flight\"\n",
        "\n",
        "# get the uri of the model\n",
        "model_uri = \"models:/\" + model_name + \"/latest\"\n",
        "\n",
        "model = mlflow.pyfunc.load_model(model_uri)\n",
        "rfModel = model._model_impl.spark_model\n",
        "\n",
        "# Make the prediction\n",
        "predictions = rfModel.transform(df)\n",
        "\n",
        "print(predictions.select(\"prediction\").toPandas())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "{\"data\": [\n",
        "        {\n",
        "            \"DAY_OF_WEEK\": 6,\n",
        "            \"SCHEDULED_ARRIVAL_sec\": 27000\n",
        "        },\n",
        "        {\n",
        "            \"DAY_OF_WEEK\": 6,\n",
        "            \"SCHEDULED_ARRIVAL_sec\": 33900\n",
        "        }\n",
        "    ]}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Ran into a deserialization error. Ignoring since this is failsafe deserialization\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1509, in failsafe_deserialize\n    return self(target_obj, data, content_type=content_type)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1375, in __call__\n    data = self._unpack_content(response_data, content_type)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1543, in _unpack_content\n    raise ValueError(\"This pipeline didn't have the RawDeserializer policy; can't deserialize\")\nValueError: This pipeline didn't have the RawDeserializer policy; can't deserialize\nRan into a deserialization error. Ignoring since this is failsafe deserialization\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1509, in failsafe_deserialize\n    return self(target_obj, data, content_type=content_type)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1375, in __call__\n    data = self._unpack_content(response_data, content_type)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1543, in _unpack_content\n    raise ValueError(\"This pipeline didn't have the RawDeserializer policy; can't deserialize\")\nValueError: This pipeline didn't have the RawDeserializer policy; can't deserialize\nRan into a deserialization error. Ignoring since this is failsafe deserialization\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1509, in failsafe_deserialize\n    return self(target_obj, data, content_type=content_type)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1375, in __call__\n    data = self._unpack_content(response_data, content_type)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1543, in _unpack_content\n    raise ValueError(\"This pipeline didn't have the RawDeserializer policy; can't deserialize\")\nValueError: This pipeline didn't have the RawDeserializer policy; can't deserialize\nRan into a deserialization error. Ignoring since this is failsafe deserialization\nTraceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1509, in failsafe_deserialize\n    return self(target_obj, data, content_type=content_type)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1375, in __call__\n    data = self._unpack_content(response_data, content_type)\n  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msrest/serialization.py\", line 1543, in _unpack_content\n    raise ValueError(\"This pipeline didn't have the RawDeserializer policy; can't deserialize\")\nValueError: This pipeline didn't have the RawDeserializer policy; can't deserialize\n2022/12/01 14:02:44 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - pyspark (current: 3.2.2, required: pyspark==3.1.3)\n - scipy (current: 1.5.3, required: scipy==1.9.3)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n2022/12/01 14:02:44 INFO mlflow.spark: File '/tmp/tmp9kgql2wp/model_delays_flight/sparkml' is already on DFS, copy is not necessary.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "   prediction\n0   -3.015961\n1   -1.833719\n"
        }
      ],
      "execution_count": 47,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {score_dir}/score.py\n",
        "import os\n",
        "import logging\n",
        "import json\n",
        "import numpy\n",
        "import joblib\n",
        "import mlflow\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "\n",
        "\n",
        "def init():\n",
        "    \"\"\"\n",
        "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
        "    You can write the logic here to perform init operations like caching the model in memory\n",
        "    \"\"\"\n",
        "    global model\n",
        "    global spark\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    model_path = os.path.join(\n",
        "        os.getenv(\"AZUREML_MODEL_DIR\"), \"model_delays_flight\"\n",
        "    )\n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = mlflow.pyfunc.load_model(model_path)\n",
        "    logging.info(\"Init complete\")\n",
        "    conf = SparkConf().setAppName(\"appName\").setMaster(\"local\")\n",
        "    sc = SparkContext.getOrCreate(conf=conf)\n",
        "    spark = SparkSession(sc)\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    \"\"\"\n",
        "    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.\n",
        "    In the example we extract the data from the json input and call the model's predict()\n",
        "    method and return the result back\n",
        "\n",
        "    :param raw_data: The input data in json format\n",
        "    :return: The prediction result in json format\n",
        "\n",
        "    the format of the input data is:\n",
        "\n",
        "    {\n",
        "        \"data\": [\n",
        "            {\n",
        "                \"DAY_OF_WEEK\": 1,\n",
        "                \"SCHEDULED_ARRIVAL_sec\": 0\n",
        "            },\n",
        "            {\n",
        "                \"DAY_OF_WEEK\": 2,\n",
        "                \"SCHEDULED_ARRIVAL_sec\": 1\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    logging.info(\"Request received\")\n",
        "    data = json.loads(raw_data)[\"data\"]\n",
        "    df = spark.createDataFrame(data)\n",
        "    # transform the data into dataframe\n",
        "    assembler = VectorAssembler(\n",
        "    inputCols=[\"DAY_OF_WEEK\", \"SCHEDULED_ARRIVAL_sec\"], outputCol=\"features\")\n",
        "\n",
        "    X_test = assembler.transform(df)\n",
        "    print(X_test.limit(10).toPandas())\n",
        "    # make prediction\n",
        "    rfModel = model._model_impl.spark_model\n",
        "    \n",
        "    result = rfModel.transform(X_test)\n",
        "\n",
        "    logging.info(\"Request processed\")\n",
        "    \n",
        "    # transform the result into list\n",
        "    return result.select(\"prediction\").toPandas().to_json(orient=\"records\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./model_scoring/score.py\n"
        }
      ],
      "execution_count": 54,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Environment, Model\n",
        "\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=\"model_delays_flight@latest\",\n",
        "    environment=\"flight-delays-custom-env@latest\",\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\"model_scoring/\", scoring_script=\"score.py\"\n",
        "    ),\n",
        "    instance_type=\"Standard_DS2_v2\",\n",
        "    instance_count=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1669885540933
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Check if the endpoint is ready\n",
        "while True:\n",
        "    if ml_client.online_endpoints.get(online_endpoint_name).provisioning_state == \"Succeeded\":\n",
        "        print(\"Endpoint is ready\")\n",
        "        ml_client.online_deployments.begin_create_or_update(blue_deployment)\n",
        "        break\n",
        "    else:\n",
        "        print(\"Endpoint is not ready\")\n",
        "        time.sleep(10)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint endpoint-12011013248391 exists\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Endpoint is ready\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading model_scoring (0.0 MBs): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2563/2563 [00:00<00:00, 229025.55it/s]\u001b[0m\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "."
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "..............................................................................."
        }
      ],
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1669885548669
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}