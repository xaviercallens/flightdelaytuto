{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PREDICTING FLIGHT DELAYS AT ARRIVAL"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Powered by the <span style=\"color:rgb(0,94,184)\">Data Science Platform and Romain*</span> <img src=\"images/logo-dsp-100x100.png\" style=\"vertical-align:middle\" width=\"25\" height=\"25\" />"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Level: <span style=\"color:green\">Beginner</span>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duration: *40 min*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to to build, train and evaluate a Random Forest model in order to predict the flight delays at arrival of Southwest airline company (WN) at the Los Angeles international airport (LAX) in 2015. The purpose is not to obtain the best possible prediction but rather to emphasize on the various steps needed to build such a model.\n",
        "\n",
        "The main goal of this notebook is to undestand all the principles of machine learning using a tool called Azure Machine Learning. We will go through every steps together to discover all the possibilities of Azure ML.\n",
        "\n",
        "This notebook will cover the features of the Data science platform below:\n",
        "* PySpark notebook\n",
        "* Data access in MaprFS Raw Data Archive using spark\n",
        "* Data preparation using SQL and spark-sql\n",
        "* Data visualization\n",
        "* Build a Random Forest model using spark-ml\n",
        "* Train this random forest model using spark-ml\n",
        "* Evaluate the model using spark-ml\n",
        "* Using Azure ML\n",
        "* Creating a component\n",
        "* Creating an environment with Dockerfile\n",
        "* Creating a Pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/global_free_trial_spark_delay.png\" width=\"700\" height=\"350\" />"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, we have to get a handle to our workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install azure-ai-ml"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: azure-ai-ml in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (1.2.0)\nRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (4.16.0)\nRequirement already satisfied: colorama<0.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (0.4.6)\nRequirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (1.3.2)\nRequirement already satisfied: azure-common<2.0.0,>=1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (1.1.28)\nRequirement already satisfied: tqdm<5.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (4.64.1)\nRequirement already satisfied: azure-storage-file-datalake<13.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (12.9.1)\nRequirement already satisfied: opencensus-ext-azure<2.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (1.1.7)\nRequirement already satisfied: azure-storage-file-share<13.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (12.10.1)\nRequirement already satisfied: msrest>=0.6.18 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (0.7.1)\nRequirement already satisfied: azure-core!=1.22.0,<2.0.0,>=1.8.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (1.26.0)\nRequirement already satisfied: azure-storage-blob<13.0.0,>=12.10.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (12.13.0)\nRequirement already satisfied: pydash<6.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (5.1.2)\nRequirement already satisfied: typing-extensions<5.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (4.4.0)\nRequirement already satisfied: strictyaml<2.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (1.6.2)\nRequirement already satisfied: marshmallow<4.0.0,>=3.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (3.19.0)\nRequirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (6.0)\nRequirement already satisfied: pyjwt<3.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (2.4.0)\nRequirement already satisfied: isodate in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (0.6.1)\nRequirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (5.10.0)\nRequirement already satisfied: pkgutil-resolve-name>=1.3.10; python_version < \"3.9\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (1.3.10)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (0.18.1)\nRequirement already satisfied: attrs>=17.4.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (22.1.0)\nRequirement already satisfied: psutil>=5.6.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (5.9.3)\nRequirement already satisfied: requests>=2.19.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (2.28.1)\nRequirement already satisfied: opencensus<1.0.0,>=0.11.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (0.11.0)\nRequirement already satisfied: azure-identity<2.0.0,>=1.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (1.7.0)\nRequirement already satisfied: cryptography>=2.1.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-storage-file-share<13.0.0->azure-ai-ml) (38.0.1)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest>=0.6.18->azure-ai-ml) (1.3.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest>=0.6.18->azure-ai-ml) (2022.9.24)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azure-ai-ml) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.6.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from strictyaml<2.0.0->azure-ai-ml) (2.8.2)\nRequirement already satisfied: packaging>=17.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.5->azure-ai-ml) (21.3)\nRequirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (3.9.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.19.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.19.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.19.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.1.1)\nRequirement already satisfied: opencensus-context>=0.1.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.1.3)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.10.2)\nRequirement already satisfied: msal-extensions~=0.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.3.1)\nRequirement already satisfied: msal<2.0.0,>=1.12.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (1.20.0)\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from cryptography>=2.1.4->azure-storage-file-share<13.0.0->azure-ai-ml) (1.15.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml) (3.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.5->azure-ai-ml) (3.0.9)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.13.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (4.21.12)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (1.56.4)\nRequirement already satisfied: portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msal-extensions~=0.3.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.6.0)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-file-share<13.0.0->azure-ai-ml) (2.21)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.2.8)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (5.2.0)\nRequirement already satisfied: pyasn1>=0.1.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.4.8)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import os\n",
        "from azure.ai.ml import MLClient\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Execute the script\n",
        "%run setenv.py\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id= os.environ['subscription_id'],\n",
        "    resource_group_name= os.environ['resource_group'],\n",
        "    workspace_name= os.environ['workspace_name']\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1671105360003
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Checking Credentials\n",
        "\n",
        "ml_client is lazy. So your credentials might be invalid. Run this cell to make sure your credentials are correct :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if credentials are valid\n",
        "from IPython.display import Image\n",
        "from colorama import Fore\n",
        "\n",
        "try :\n",
        "    ml_client.begin_create_or_update(ml_client.workspaces.get())\n",
        "    print(Fore.GREEN + \"Credentials are valid\")\n",
        "except :\n",
        "    print(Fore.RED + \"Credentials are invalid - please check the TODO CELL\")\n",
        "    print(\"Please check your credentials : subscription_id, resource_group_name, workspace_name must be correct\")\n",
        "    display(Image(filename='images/credentials.PNG'))\n",
        "    print(\"You can find your credentials by clicking on the TOP LEFT of the Azure Portal ML Studio\")\n",
        "    display(Image(filename='images/values.png'))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[32mCredentials are valid\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1671105362566
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a compute resource to run your job\n",
        "\n",
        "You'll need a compute resource for running a job. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n",
        "\n",
        "You'll provision a Linux compute cluster. See the [full list on VM sizes and prices](https://azure.microsoft.com/pricing/details/machine-learning/) .\n",
        "\n",
        "For this example, you only need a basic cluster, so you'll use a Standard_DS11_v2 model and create an Azure ML Compute."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster-flights\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_DS11_V2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=4,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    \n",
        "\n",
        "print(\n",
        "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named cpu-cluster-flights, we'll reuse it as is.\nAMLCompute with name cpu-cluster-flights is created, the compute size is STANDARD_DS11_V2\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1671105364553
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Create Environnment for our execution/job"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run your AzureML job on your compute resource, you'll need an environment. An environment lists the software runtime and libraries that you want installed on the compute where you’ll be training. It's similar to your python environment on your local machine.\n",
        "\n",
        "AzureML provides many curated or ready-made environments, which are useful for common training and inference scenarios. You can also create your own custom environments using a docker image, or a conda configuration.\n",
        "\n",
        "In this example, you'll create a custom conda environment for your jobs, using a conda yaml file.\n",
        "\n",
        "First, create a directory to store the file in."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "env_dir = \"./env\"\n",
        "os.makedirs(env_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1671105428131
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create the Dockerfile where our pipeline will run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {env_dir}/Dockerfile\n",
        "FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20220902.v1\n",
        "\n",
        "ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/responsibleai-0.21\n",
        "\n",
        "# Install wkhtmltopdf for pdf rendering from html\n",
        "RUN apt-get -y update && apt-get -y install wkhtmltopdf\n",
        "\n",
        "# Create conda environment\n",
        "RUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n",
        "    python=3.8 pip=21.3.1 -c anaconda -c conda-forge\n",
        "\n",
        "# Prepend path to AzureML conda environment\n",
        "ENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\n",
        "\n",
        "# Install pip dependencies\n",
        "# markupsafe and itsdangerous are bug workarounds\n",
        "# install azureml-defaults==1.47.0\n",
        "    # inference-schema[numpy-support]==1.5\n",
        "    # joblib==1.0.1\n",
        "RUN pip install 'responsibleai~=0.21.0' \\\n",
        "                'raiwidgets~=0.21.0' \\\n",
        "                'pyarrow' \\\n",
        "                'markupsafe<=2.0.1' \\\n",
        "                'itsdangerous==2.0.1' \\\n",
        "                'mlflow==1.30.0' \\\n",
        "                'scikit-learn<1.1' \\\n",
        "                'pdfkit==1.0.0' \\\n",
        "                'plotly==5.6.0' \\\n",
        "                'kaleido==0.2.1' \\\n",
        "                'azureml-core==1.47.0' \\\n",
        "                'azureml-dataset-runtime==1.47.0' \\\n",
        "                'azureml-mlflow==1.47.0' \\\n",
        "                'azureml-telemetry==1.47.0'\\\n",
        "                'seaborn'\\\n",
        "                'matplotlib'\\\n",
        "                'pyspark>=3.1,<3.2'\\\n",
        "                'azureml-defaults==1.47.0'\\\n",
        "                'inference-schema[numpy-support]==1.5'\\\n",
        "                'joblib==1.0.1'\n",
        "                \n",
        "                    \n",
        "\n",
        "RUN pip install --pre 'azure-ai-ml'\n",
        "\n",
        "# no-deps install for domonic due to unresolvable dependencies requirment on urllib3 and requests.\n",
        "# score card rendering is using domonic only for the html elements composer which does not involve requests or urllib3\n",
        "RUN pip install --no-deps 'charset-normalizer==2.0.12' \\\n",
        "                          'cssselect==1.1.0' \\\n",
        "                          'elementpath==2.5.0' \\\n",
        "                          'html5lib==1.1' \\\n",
        "                          'webencodings==0.5.1' \\\n",
        "                          'domonic==0.9.10'\n",
        "\n",
        "# This is needed for mpi to locate libpython\n",
        "ENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH/lib:$LD_LIBRARY_PATH\n",
        "\n",
        "# This is needed for pyspark to locate Java\n",
        "RUN apt-get update && \\\n",
        "    mkdir -p /usr/share/man/man1 && \\\n",
        "    apt-get install -y openjdk-8-jdk && \\\n",
        "    apt-get install -y ant && \\\n",
        "    apt-get clean && \\\n",
        "    rm -rf /var/lib/apt/lists/ && \\\n",
        "    rm -rf /var/cache/oracle-jdk8-installer;\n",
        "    \n",
        "ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\n",
        "RUN export JAVA_HOME\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./env/Dockerfile\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use a job to register our environment into our workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "from azure.ai.ml.entities import BuildContext\n",
        "import os\n",
        "\n",
        "custom_env_name = \"flight-delays-custom-env\"\n",
        "\n",
        "buildcontext = BuildContext(\n",
        "    path=env_dir\n",
        ")\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for spark flight delays\",\n",
        "    tags={\"owner\": os.environ[\"owner\"], \"created\": \"2022-11-23\"},\n",
        "    build=buildcontext,\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading env (0.0 MBs):   0%|          | 0/2484 [00:00<?, ?it/s]\r\u001b[32mUploading env (0.0 MBs): 100%|██████████| 2484/2484 [00:00<00:00, 246531.11it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name flight-delays-custom-env is registered to workspace, the environment version is 1\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1671105512718
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Writing our component\n",
        "First of all, we are creating a yml file. This file will be a description of our azure ml component. It explains how this component works."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "component_dir = \"./components\"\n",
        "os.makedirs(component_dir, exist_ok=True)\n",
        "\n",
        "analysis_src_dir = \"./components/src\"\n",
        "os.makedirs(analysis_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1671105712981
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $component_dir/flight_delays_analysis.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: analysis\n",
        "display_name: analysis\n",
        "version: 30\n",
        "type: command\n",
        "inputs:\n",
        "  public_data_flight_delays: \n",
        "    type: uri_file\n",
        "outputs:\n",
        "  eval_output:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:flight-delays-custom-env@latest\n",
        "command: >-\n",
        "  python analysis.py\n",
        "  --public_data_flight_delays ${{inputs.public_data_flight_delays}} \n",
        "  --eval_output ${{outputs.eval_output}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./components/flight_delays_analysis.yml\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1669192288076
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to actually create the code that will be executed by our component"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {analysis_src_dir}/analysis.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import datetime, warnings, scipy \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "from collections import OrderedDict\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pathlib import Path\n",
        "\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--public_data_flight_delays\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--eval_output\", type=str, help=\"path to eval output\")\n",
        "    args = parser.parse_args()\n",
        "   \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    ###################\n",
        "    #<load the data>\n",
        "    ###################\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.public_data_flight_delays)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    flights = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays, \"flights.csv\"), header=True)\n",
        "    airlines_names = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays,\"airlines.csv\"), header=True).toPandas()\n",
        "    \n",
        "    ####################\n",
        "    #</load the data>\n",
        "    ####################\n",
        "\n",
        "    ####################\n",
        "    #<Metrics rows and columns>\n",
        "    ####################\n",
        "    mlflow.log_metric(\"nb_rows\", flights.count())\n",
        "    mlflow.log_metric(\"nb_columns\", len(flights.columns))\n",
        "\n",
        "    ####################\n",
        "    #</Metrics rows and columns>\n",
        "    ####################\n",
        "\n",
        "    ####################\n",
        "    #<sample csv load>\n",
        "    ####################\n",
        "    tab_info=pd.DataFrame(flights.dtypes).T.rename(index={0:'column type'})\n",
        "\n",
        "    sample = flights.limit(10).toPandas()\n",
        "\n",
        "    sample.to_csv(\"sample.csv\")\n",
        "\n",
        "    mlflow.log_artifact(\"sample.csv\")\n",
        "\n",
        "    flights.createOrReplaceTempView(\"flights\")\n",
        "\n",
        "    airline_delays = spark.sql( \\\n",
        "        \"SELECT AIRLINE, MIN(ARRIVAL_DELAY) as min, MAX(ARRIVAL_DELAY) as max, AVG(ARRIVAL_DELAY) as avg, count(*) as count \\\n",
        "        from flights WHERE MONTH == 1 AND ARRIVAL_DELAY is not null \\\n",
        "        group by AIRLINE order by count\")\n",
        "\n",
        "    tab_pandas = airline_delays.toPandas()\n",
        "    \n",
        "    tab_pandas.to_csv(\"airline_delays.csv\")\n",
        "\n",
        "    mlflow.log_artifact(\"airline_delays.csv\")\n",
        "\n",
        "    ####################\n",
        "    #</sample csv load>\n",
        "    ####################\n",
        "\n",
        "\n",
        "    ####################\n",
        "    #<Graph Visualisation>\n",
        "    ####################\n",
        "    # 1.6 Flight & Mean Flight Delay distribution per Airline\n",
        "\n",
        "    abbr_companies = airlines_names.set_index('IATA_CODE')['AIRLINE'].to_dict()\n",
        "    global_stats = airline_delays.toPandas()\n",
        "    font = {'family' : 'normal', 'weight' : 'bold', 'size'   : 15}\n",
        "    mpl.rc('font', **font)\n",
        "    #__________________________________________________________________\n",
        "    colors = ['royalblue', 'grey', 'wheat', 'c', 'firebrick', 'seagreen', 'lightskyblue',\n",
        "            'lightcoral', 'yellowgreen', 'gold', 'tomato', 'violet', 'aquamarine', 'chartreuse']\n",
        "    #___________________________________\n",
        "    fig = plt.figure(1, figsize=(16,15))\n",
        "    gs=GridSpec(2,2)             \n",
        "    ax1=fig.add_subplot(gs[0,0]) \n",
        "    ax2=fig.add_subplot(gs[0,1])  \n",
        "    #------------------------------\n",
        "    # Pie chart nº1: nb of flights\n",
        "    #------------------------------\n",
        "    labels = [s for s in  global_stats.index]\n",
        "    sizes  = global_stats['count'].values\n",
        "    explode = [0.3 if sizes[i] < 20000 else 0.0 for i in range(len(abbr_companies))]\n",
        "    patches, texts, autotexts = ax1.pie(sizes, explode = explode,\n",
        "                                    labels=labels, colors = colors,  autopct='%1.0f%%',\n",
        "                                    shadow=False, startangle=0)\n",
        "    for i in range(len(abbr_companies)): \n",
        "        texts[i].set_fontsize(14)\n",
        "    ax1.axis('equal')\n",
        "    ax1.set_title('% of flights per company', bbox={'facecolor':'midnightblue', 'pad':5},\n",
        "                color = 'w',fontsize=18)\n",
        "    #_______________________________________________\n",
        "    # I set the legend: abreviation -> airline name\n",
        "    comp_handler = []\n",
        "    i = 0\n",
        "    for company in abbr_companies:\n",
        "        comp_handler.append(mpatches.Patch(color=colors[i],\n",
        "                label = abbr_companies[company]))\n",
        "        i = i + 1\n",
        "    ax1.legend(handles=comp_handler, bbox_to_anchor=(0.2, 0.9), \n",
        "            fontsize = 13, bbox_transform=plt.gcf().transFigure)\n",
        "    #----------------------------------------\n",
        "    # Pie chart nº2: mean delay at arrival\n",
        "    #----------------------------------------\n",
        "    sizes  = global_stats['avg'].values\n",
        "    sizes  = [max(s,0) for s in sizes]\n",
        "    explode = [0.0 if sizes[i] < 20000 else 0.01 for i in range(len(abbr_companies))]\n",
        "    patches, texts, autotexts = ax2.pie(sizes, explode = explode, labels = labels,\n",
        "                                    colors = colors, shadow=False, startangle=0,\n",
        "                                    autopct = lambda p :  '{:.0f}'.format(p * sum(sizes) / 100))\n",
        "    for i in range(len(abbr_companies)): \n",
        "        texts[i].set_fontsize(14)\n",
        "    ax2.axis('equal')\n",
        "    ax2.set_title('Mean delay at arrival', bbox={'facecolor':'midnightblue', 'pad':5},\n",
        "                color='w', fontsize=18)\n",
        "    #________________________\n",
        "    plt.tight_layout(w_pad=3)\n",
        "    plt.subplots()\n",
        "    #________________________\n",
        "    mlflow.log_figure(fig, \"flight_delay_per_airline.png\")\n",
        "\n",
        "    ####################\n",
        "    #</Graph Visualisation>\n",
        "    ####################\n",
        "\n",
        "    # 1.7. How airport impacts delays - Mean delays at arrival for each airport for Southwest airline\n",
        "\n",
        "    # Reset plot\n",
        "    plt.clf()\n",
        "\n",
        "    airline = \"'WN'\" # South West\n",
        "\n",
        "    airports_delays = spark.sql( \\\n",
        "    \"SELECT DESTINATION_AIRPORT, MIN(ARRIVAL_DELAY) as min, MAX(ARRIVAL_DELAY) as max, AVG(ARRIVAL_DELAY) as avg, count(*) as count \\\n",
        "    from flights WHERE MONTH == 1 AND AIRLINE == \" + airline + \" AND ARRIVAL_DELAY is not null \\\n",
        "    group by DESTINATION_AIRPORT order by DESTINATION_AIRPORT\")\n",
        "\n",
        "    airports_delays.limit(10).toPandas().to_csv(\"airports_delays.csv\")\n",
        "    mlflow.log_artifact(\"airports_delays.csv\")\n",
        "    \n",
        "\n",
        "    # Output of our component\n",
        "    eval_msg = f\"Eval done\\n\"\n",
        "    (Path(args.eval_output) / \"eval_result.txt\").write_text(eval_msg)\n",
        "   \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./components/src/analysis.py\n"
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Creating our component with a command\n",
        "* Open a terminal\n",
        "\n",
        "<img src=\"images/openTerminal.PNG\" width=\"800\" height=\"600\" />\n",
        "\n",
        "* Go into the right folder\n",
        "```\n",
        "$ cd flightdelaytuto/tutoAzureML/\n",
        "```\n",
        "* Execute the setenv.sh script that will setup variables environment\n",
        "```\n",
        "$ source setenv.sh\n",
        "```\n",
        "* Go into the components folder \n",
        "```\n",
        "$ cd components\n",
        "```\n",
        "* Login Azure with this command\n",
        "```\n",
        "$ az login --tenant $tenant_id\n",
        "```\n",
        "* After you successfuly login, set the right subscription ID you are currently using in Microsoft Azure Machine learning Studio.\n",
        "```\n",
        "$ az account set --subscription $subscription_id\n",
        "```\n",
        "* Set your workspace and resource group\n",
        "```\n",
        "$ az configure --defaults workspace=$workspace_name group=$resource_group\n",
        "```\n",
        "* You are now perfectly set up and can create a component with this command with the .yml file that we created earlier :\n",
        "```\n",
        "$ az ml component create --file flight_delays_analysis.yml\n",
        "```\n",
        "* It should display a JSON with informations of the component you just created"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Creating a pipeline with the designer tool\n",
        "\n",
        "Open a new window of Azure ML Studio\n",
        "\n",
        "Go to Pipelines and create a new pipeline drafts\n",
        "\n",
        "Select Custom using custom components\n",
        "\n",
        "You should now see the designer tool\n",
        "\n",
        "Now select in Data, the dataset-delays-flights and drag it into the pipeline model\n",
        "\n",
        "Select Component and drop analysis component into the pipeline model\n",
        "\n",
        "Now drag the Data output port into public_flight_delays input port\n",
        "\n",
        "Rename the pipeline with the name : **Analysis Delay Flights**\n",
        "\n",
        "Go to settings and Select a Compute Cluster type and select : **cpu-cluster-flights** that we create earlier\n",
        "\n",
        "Your screen should look like this :\n",
        "\n",
        "<img src=\"images/designer_tool.PNG\" width=\"800\" height=\"390\" />\n",
        "\n",
        "It is time to press Submit (Create a new experiment called ExperimentDelayFlight) to launch this pipeline for the first time\n",
        "\n",
        "**Note that a .yml file can completely replace the design tool**\n",
        "\n",
        "In the nav bar : Assets : Jobs on the left you should see your job running (30 seconds after the submit)\n",
        "\n",
        "Click on the Job and wait for the Job to finish running (It can take 5 up to 15 minutes to run ,installing environment on a node...)\n",
        "\n",
        "While waiting, you can start creating the second component in part 2\n",
        "\n",
        "When it is complete, click on the analysis component\n",
        "Pipeline job overview is where you should see your pipeline running\n",
        "\n",
        "<img src=\"images/overview.PNG\" width=\"700\" height=\"600\" />\n",
        "\n",
        "**Important**\n",
        "Whenever the job is complete, you can see a lot of informations in the component analysis that has finished.\n",
        "On the top nav bar :\n",
        "\n",
        "-Output + Logs = when you see the csv that has been generate by the pipeline and the output file\n",
        "\n",
        "-Metrics : Values that have been registered such as nb of rows\n",
        "* 5,819,079 rows\n",
        "* 31 columns\n",
        "\n",
        "-Images : Images that have been loaded during the execution"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Model training: Single airport LAX, Single airline WN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previsous sections dealt with an exploration of the dataset.  \n",
        "From here, we start with the modeling of flight delays. \n",
        "\n",
        "**Motivations**\n",
        "\n",
        "There is a high variability in average delays, both between the different airports but also between the different airlines. So, it is necessary to learn model that is specific to an airline the destination airport.\n",
        "\n",
        "We will learn model that predicts the flight delays at the destination airport at a given time of arrival (on January). We will work on Southwest Airlines (WN) flights arriving at the airport of Los Angeles International Airport (LAX). We will use the 3 first weeks of January as the training set and the follwoing week of January as the test set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Training Component\n",
        "\n",
        "Now we create our second component which will create a model and train the model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $component_dir/flight_delays_training.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: train_delay_model\n",
        "display_name: train_delay_model\n",
        "version: 11\n",
        "type: command\n",
        "inputs:\n",
        "  public_data_flight_delays: \n",
        "    type: uri_file\n",
        "  num_trees:\n",
        "    type: integer\n",
        "    default: 100\n",
        "outputs:\n",
        "  eval_output:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:flight-delays-custom-env@latest\n",
        "command: >-\n",
        "  python train.py\n",
        "  --public_data_flight_delays ${{inputs.public_data_flight_delays}} \n",
        "  --num_trees ${{inputs.num_trees}}\n",
        "  --eval_output ${{outputs.eval_output}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./components/flight_delays_training.yml\n"
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the code of our component"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {analysis_src_dir}/train.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import datetime, warnings, scipy \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "from collections import OrderedDict\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pathlib import Path\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "\n",
        "def format_date(year, month, day, scheduled):\n",
        "    if scheduled == 2400: \n",
        "        scheduled = 0\n",
        "    scheduled = \"{0:04d}\".format(int(scheduled))\n",
        "    return datetime.datetime(year, month, day, int(scheduled[0:2]), int(scheduled[2:4])).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def format_hour(scheduled):\n",
        "    if scheduled == 2400: \n",
        "        scheduled = 0\n",
        "    scheduled = \"{0:04d}\".format(int(scheduled))\n",
        "    return int(scheduled[0:2])\n",
        "    \n",
        "def format_seconds(scheduled):\n",
        "    if scheduled == 2400: \n",
        "        scheduled = 0\n",
        "    scheduled = \"{0:04d}\".format(int(scheduled))\n",
        "    return (3600 * int(scheduled[0:2])) + (60 * int(scheduled[2:4]))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--public_data_flight_delays\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--num_trees\", type=int, required=False, default=100)\n",
        "    parser.add_argument(\"--eval_output\", type=str, help=\"path to eval output\")\n",
        "    args = parser.parse_args()\n",
        "   \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    ###################\n",
        "    #<load the data>\n",
        "    ###################\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.public_data_flight_delays)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    flights = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays, \"flights.csv\"), header=True)\n",
        "    airlines_names = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays,\"airlines.csv\"), header=True).toPandas()\n",
        "    \n",
        "    ####################\n",
        "    #</load the data>\n",
        "    ####################\n",
        "    udf_format_date = F.udf(format_date, StringType())\n",
        "    udf_format_hour = F.udf(format_hour, IntegerType())\n",
        "    udf_format_seconds = F.udf(format_seconds, IntegerType())\n",
        "    flights = flights \\\n",
        "        .withColumn('SCHEDULED_DEPARTURE_FORMATTED',udf_format_date(flights.YEAR, flights.MONTH, flights.DAY, flights.SCHEDULED_DEPARTURE)) \\\n",
        "        .withColumn('SCHEDULED_ARRIVAL_FORMATTED',udf_format_date(flights.YEAR, flights.MONTH, flights.DAY, flights.SCHEDULED_ARRIVAL)) \\\n",
        "        .withColumn('SCHEDULED_ARRIVAL_hour',udf_format_hour(flights.SCHEDULED_ARRIVAL)) \\\n",
        "        .withColumn('SCHEDULED_ARRIVAL_sec',udf_format_seconds(flights.SCHEDULED_ARRIVAL))\n",
        "\n",
        "    flights.limit(10).toPandas().to_csv(\"flights.csv\")\n",
        "    mlflow.log_artifact(\"flights.csv\")\n",
        "\n",
        "    airline = \"'WN'\"\n",
        "    airport = \"'LAX'\"\n",
        "\n",
        "    flights.createOrReplaceTempView(\"flights\")\n",
        "\n",
        "    df_fi = spark.sql( \\\n",
        "        \"SELECT DAY_OF_WEEK, SCHEDULED_ARRIVAL_sec, DAY, ARRIVAL_DELAY as label \\\n",
        "        from flights WHERE AIRLINE == \" + airline + \" AND DESTINATION_AIRPORT == \" + airport +  \" AND ARRIVAL_DELAY is not null AND MONTH == 1\")\n",
        "\n",
        "    df_fi.limit(10).toPandas().to_csv(\"df_fi.csv\")\n",
        "    mlflow.log_artifact(\"df_fi.csv\")\n",
        "\n",
        "    df_train = df_fi.filter(F.col('DAY') < 23).drop('DAY')\n",
        "    df_test = df_fi.filter(F.col('DAY') > 23).drop('DAY')\n",
        "    print(\"training dataset size: \" + str(df_train.count()))\n",
        "    print(\"test dataset size: \" + str(df_test.count()))\n",
        "    df_train.limit(10).toPandas().to_csv(\"df_train.csv\")\n",
        "    mlflow.log_artifact(\"df_train.csv\")\n",
        "\n",
        "    assembler = VectorAssembler(\n",
        "    inputCols=[\"DAY_OF_WEEK\", \"SCHEDULED_ARRIVAL_sec\"], outputCol=\"features\"\n",
        "    )\n",
        "\n",
        "    X_train = assembler.transform(df_train)\n",
        "    X_train.limit(10).toPandas().to_csv(\"X_train.csv\")\n",
        "    mlflow.log_artifact(\"X_train.csv\")\n",
        "\n",
        "    # 2.2.1 Build, Train Model and visualize feature importances\n",
        "    # Train a RandomForest model\n",
        "    rf = RandomForestRegressor(numTrees=args.num_trees, featuresCol='features',labelCol='label',predictionCol='prediction')\n",
        "    rfModel = rf.fit(X_train)\n",
        "\n",
        "    importances = rfModel.featureImportances\n",
        "\n",
        "    x_values = list(range(len(importances)))\n",
        "\n",
        "    # Visualize the feature importances\n",
        "    plt.bar(x_values, importances, orientation = 'vertical')\n",
        "    plt.xticks(x_values, [\"DAY_OF_WEEK\", \"SCHEDULED_ARRIVAL_sec\"], rotation=40)\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xlabel('Feature')\n",
        "    plt.title('Feature Importances')\n",
        "\n",
        "    plt.savefig(\"feature_importances.png\")\n",
        "    mlflow.log_artifact(\"feature_importances.png\")\n",
        "\n",
        "    # 2.3 Model evaluation\n",
        "    X_test = assembler.transform(df_test)\n",
        "    predictions = rfModel.transform(X_test)\n",
        "    predictions.select(\"prediction\", \"label\", \"features\").limit(10).toPandas().to_csv(\"predictions.csv\")\n",
        "    mlflow.log_artifact(\"predictions.csv\")\n",
        "\n",
        "    evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "    mse = evaluator.evaluate(predictions)\n",
        "    print(\"Mean Squared Error (MSE) on test data = %g\" % mse)\n",
        "\n",
        "    mlflow.log_metric(\"mse\", mse)\n",
        "\n",
        "    # 2.4 Model registry\n",
        "\n",
        "    # get the current run id\n",
        "    run_id = mlflow.active_run().info.run_id\n",
        "\n",
        "    # Save the model\n",
        "    mlflow.spark.log_model(rfModel, \"model_delays_flight\")\n",
        "    mlflow.spark.save_model(rfModel, \"model_delays_flight\")\n",
        "    \n",
        "    mlflow.register_model(\"runs:/\" + run_id + \"/model_delays_flight\", \"model_delays_flight\")\n",
        "\n",
        "\n",
        "    # Output of our component\n",
        "    eval_msg = f\"Eval done\\n\"\n",
        "    (Path(args.eval_output) / \"eval_result.txt\").write_text(eval_msg)\n",
        "   \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/src/train.py\n"
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Creating an other component with a command\n",
        "* Open a terminal\n",
        "\n",
        "<img src=\"images/openTerminal.PNG\" width=\"800\" height=\"600\" />\n",
        "\n",
        "* Go into the right folder\n",
        "```\n",
        "$ cd flightdelaytuto/tutoAzureML/\n",
        "```\n",
        "* Execute the setenv.sh script that will setup variables environment\n",
        "```\n",
        "$ source setenv.sh\n",
        "```\n",
        "* Go into the components folder \n",
        "```\n",
        "$ cd components\n",
        "```\n",
        "Now we should already been logged. If there is any problem go to the login instructions above\n",
        "```\n",
        "$ az ml component create --file flight_delays_training.yml\n",
        "```\n",
        "* It should display a JSON with informations of the component you just created"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Updating our pipeline with the designer tool\n",
        "\n",
        "* Open a new window of Azure ML Studio\n",
        "\n",
        "* Go to Pipelines on the left nav bar\n",
        "\n",
        "* Select Pipeline Drafts and modify the pipeline that you created earlier (Click on it)\n",
        "\n",
        "* You should now see the designer tool\n",
        "\n",
        "* Select Component and drop train_delay_model component into the pipeline model\n",
        "\n",
        "* Now drag the Data output port into public_flight_delays input port from train_delay_model component\n",
        "\n",
        "* Rename the pipeline with the name : **Analysis and Train Delay Flights**\n",
        "\n",
        "* Click on the train_delay_model component, you should be able to see a parameters called num_trees which is used by our component when it trains our model with randomForest algorithm\n",
        "\n",
        "\n",
        "* It is time to press Submit to launch this pipeline and select the same experiment\n",
        "\n",
        "* Now you can see that analysis composent finished instantly because all the steps have been reused fron the precedent execution\n",
        "\n",
        "(It can take 5 up to 15 minutes to run)\n",
        "\n",
        "**Note that a .yml file can completely replace the design tool**\n",
        "\n",
        "**Important**\n",
        "Whenever the job is complete, you can see a lot of informations in the component analysis/train_delay_model that has finished :\n",
        "\n",
        "-Output + Logs = when you see the csv that has been generate by the pipeline and the output file\n",
        "\n",
        "-Metrics : Values that have been registered such as nb of rows\n",
        "* 5,819,079 rows\n",
        "* 31 columns\n",
        "\n",
        "-Images : Images that have been loaded during the execution"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create endpoint and deployment\n",
        "\n",
        "Online endpoints are endpoints that are used for online (real-time) inferencing\n",
        "Our goal here is to deploy our model so everyone can use this model to get a prediction.\n",
        "Endpoint is a server that runs into Azure CPU or GPU. You can request a prediction with REST API when it deploys\n",
        "\n",
        "## 2.1 Create endpoint"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we create an empty endpoint. This is like an empty shell"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a local endpoint\n",
        "import datetime\n",
        "import os\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        ")\n",
        "\n",
        "online_endpoint_name = \"EndptDFlt\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "print(online_endpoint_name)\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is an online endpoint\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\"owner\": os.environ[\"owner\"]},\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "EndpointDelayFlight20221215125942\n"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'owner'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(online_endpoint_name)\n\u001b[1;32m     16\u001b[0m \u001b[39m# create an online endpoint\u001b[39;00m\n\u001b[1;32m     17\u001b[0m endpoint \u001b[39m=\u001b[39m ManagedOnlineEndpoint(\n\u001b[1;32m     18\u001b[0m     name\u001b[39m=\u001b[39monline_endpoint_name,\n\u001b[1;32m     19\u001b[0m     description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthis is an online endpoint\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     auth_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m---> 21\u001b[0m     tags\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mowner\u001b[39m\u001b[39m\"\u001b[39m: os\u001b[39m.\u001b[39;49menviron[\u001b[39m\"\u001b[39;49m\u001b[39mowner\u001b[39;49m\u001b[39m\"\u001b[39;49m]},\n\u001b[1;32m     22\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/os.py:675\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    672\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)]\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[39m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodevalue(value)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'owner'"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1671109964287
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create this endpoint into our workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_create_or_update(endpoint)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "<azure.core.polling._poller.LROPoller at 0x7f5a76de5310>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1671107461098
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's make a new directory for our script that will get the data, predict and return the prediction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "score_dir = \"./score\"\n",
        "os.makedirs(score_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1671107487773
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we put our score.py into our directory.\n",
        "This score.py always have an init function that will be call only once when we deploy this script\n",
        "Beside this function, we also have a run that is called everytime a prediction has to be made\n",
        "Here we can see that init just loads the model and run gather the data, use the model to predict and returns the prediction. This is exactly what we expected from en endpoint deployment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {score_dir}/score.py\n",
        "import os\n",
        "import logging\n",
        "import json\n",
        "import numpy\n",
        "import joblib\n",
        "import mlflow\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "\n",
        "\n",
        "def init():\n",
        "    \"\"\"\n",
        "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
        "    You can write the logic here to perform init operations like caching the model in memory\n",
        "    \"\"\"\n",
        "    global model\n",
        "    global spark\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    model_path = os.path.join(\n",
        "        os.getenv(\"AZUREML_MODEL_DIR\"), \"model_delays_flight\"\n",
        "    )\n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = mlflow.pyfunc.load_model(model_path)\n",
        "    logging.info(\"Init complete\")\n",
        "    conf = SparkConf().setAppName(\"appName\").setMaster(\"local\")\n",
        "    sc = SparkContext.getOrCreate(conf=conf)\n",
        "    spark = SparkSession(sc)\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    \"\"\"\n",
        "    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.\n",
        "    In the example we extract the data from the json input and call the model's predict()\n",
        "    method and return the result back\n",
        "\n",
        "    :param raw_data: The input data in json format\n",
        "    :return: The prediction result in json format\n",
        "\n",
        "    the format of the input data is:\n",
        "\n",
        "    {\n",
        "        \"data\": [\n",
        "            {\n",
        "                \"DAY_OF_WEEK\": 1,\n",
        "                \"SCHEDULED_ARRIVAL_sec\": 0\n",
        "            },\n",
        "            {\n",
        "                \"DAY_OF_WEEK\": 2,\n",
        "                \"SCHEDULED_ARRIVAL_sec\": 1\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    logging.info(\"Request received\")\n",
        "    data = json.loads(raw_data)[\"data\"]\n",
        "    df = spark.createDataFrame(data)\n",
        "    # transform the data into dataframe\n",
        "    assembler = VectorAssembler(\n",
        "    inputCols=[\"DAY_OF_WEEK\", \"SCHEDULED_ARRIVAL_sec\"], outputCol=\"features\")\n",
        "\n",
        "    X_test = assembler.transform(df)\n",
        "    print(X_test.limit(10).toPandas())\n",
        "    # make prediction\n",
        "    rfModel = model._model_impl.spark_model\n",
        "    \n",
        "    result = rfModel.transform(X_test)\n",
        "\n",
        "    logging.info(\"Request processed\")\n",
        "    \n",
        "    # transform the result into list\n",
        "    return result.select(\"prediction\").toPandas().to_json(orient=\"records\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./score/score.py\n"
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We create an object that represent our online deployment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Environment, Model\n",
        "\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=\"model_delays_flight@latest\",\n",
        "    environment=\"flight-delays-custom-env@latest\",\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\"score/\", scoring_script=\"score.py\"\n",
        "    ),\n",
        "    instance_type=\"Standard_DS2_v2\",\n",
        "    instance_count=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1671108771932
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* And now we deploy into our endpoint (Our endpoint has to be fully created to get a deployment)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Check if the endpoint is ready\n",
        "while True:\n",
        "    if ml_client.online_endpoints.get(online_endpoint_name).provisioning_state == \"Succeeded\":\n",
        "        print(\"Endpoint is ready\")\n",
        "        ml_client.online_deployments.begin_create_or_update(blue_deployment)\n",
        "        break\n",
        "    else:\n",
        "        print(\"Endpoint is not ready, waiting...\")\n",
        "        time.sleep(10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint EndpointDelayFlight25 exists\n\u001b[32mUploading score (0.0 MBs): 100%|██████████| 2878/2878 [00:00<00:00, 65881.16it/s]\n\u001b[39m\n\ndata_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1671108779051
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To see if is completed, go on the nav bar on the left and select Endpoints\n",
        "\n",
        "<img src=\"images/endpoint.PNG\" width=\"850\" height=\"420\" />\n",
        "\n",
        "* Select the endpoint that you created\n",
        "\n",
        "* It should take 5 minutes to deploy blue wich is our deployment\n",
        "\n",
        "* When it is done, select Test on the top nav bar\n",
        "\n",
        "<img src=\"images/testEndpoint.PNG\" width=\"700\" height=\"420\" />\n",
        "\n",
        "* Now put this data into the test and press Test :\n",
        "```\n",
        " {\n",
        "        \"data\": [\n",
        "            {\n",
        "                \"DAY_OF_WEEK\": 6,\n",
        "                \"SCHEDULED_ARRIVAL_sec\": 39000\n",
        "            },\n",
        "            {\n",
        "                \"DAY_OF_WEEK\": 2,\n",
        "                \"SCHEDULED_ARRIVAL_sec\": 38000\n",
        "            }\n",
        "        ]\n",
        "} \n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Congratulations\n",
        "\n",
        "You've used the Data Science Platform for an exploratory usecase where you analyse a data set and build a machine learning model to predict flight delays at arrival.\n",
        "\n",
        "**You are now**\n",
        "\n",
        "  ✔️ AzureML Grand Master\n",
        "\n",
        "**IMPORTANT =>**\n",
        "\n",
        "⚠️ Don't forget to shutdown/delete the compute that you created (compute instance + compute cluster [Compute on the left nav bar] + endpoint)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
