{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PREDICTING FLIGHT DELAYS AT ARRIVAL"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Powered by the <span style=\"color:rgb(0,94,184)\">Data Science Platform and Romain*</span> <img src=\"images/logo-dsp-100x100.png\" style=\"vertical-align:middle\" width=\"25\" height=\"25\" />"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Level: <span style=\"color:green\">Beginner</span>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duration: *40 min*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to to build, train and evaluate a Random Forest model in order to predict the flight delays at arrival of Southwest airline company (WN) at the Los Angeles international airport (LAX) in 2015. The purpose is not to obtain the best possible prediction but rather to emphasize on the various steps needed to build such a model.\n",
        "\n",
        "The main goal of this notebook is to undestand all the principles of machine learning using a tool called Azure Machine Learning. We will go through every steps together to discover all the possibilities of Azure ML.\n",
        "\n",
        "This notebook will cover the features of the Data science platform below:\n",
        "* PySpark notebook\n",
        "* Data access in MaprFS Raw Data Archive using spark\n",
        "* Data preparation using SQL and spark-sql\n",
        "* Data visualization\n",
        "* Build a Random Forest model using spark-ml\n",
        "* Train this random forest model using spark-ml\n",
        "* Evaluate the model using spark-ml\n",
        "* Using Azure ML\n",
        "* Creating a component\n",
        "* Creating an environment with Dockerfile\n",
        "* Creating a Pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/global_free_trial_spark_delay.png\" width=\"700\" height=\"350\" />"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, we have to get a handle to our workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle to the workspace\n",
        "%pip install azure-ai-ml\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"8b5374c5-8b98-45fb-bd96-7d5a4ce4e527\",  # this will look like xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n",
        "    resource_group_name=\"rg-sbx-aiops\",\n",
        "    workspace_name=\"romain-mlbox\",\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting azure-ai-ml\n  Downloading azure_ai_ml-1.1.2-py3-none-any.whl (4.0 MB)\n\u001b[K     |████████████████████████████████| 4.0 MB 4.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: azure-storage-blob<13.0.0,>=12.10.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (12.13.0)\nCollecting azure-storage-file-datalake<13.0.0\n  Downloading azure_storage_file_datalake-12.9.1-py3-none-any.whl (238 kB)\n\u001b[K     |████████████████████████████████| 238 kB 95.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (4.16.0)\nRequirement already satisfied: azure-core!=1.22.0,<2.0.0,>=1.8.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (1.26.0)\nRequirement already satisfied: colorama<0.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (0.4.6)\nRequirement already satisfied: isodate in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (0.6.1)\nRequirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (6.0)\nCollecting azure-storage-file-share<13.0.0\n  Downloading azure_storage_file_share-12.10.1-py3-none-any.whl (252 kB)\n\u001b[K     |████████████████████████████████| 252 kB 95.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: azure-common<2.0.0,>=1.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (1.1.28)\nRequirement already satisfied: pyjwt<3.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (2.4.0)\nRequirement already satisfied: typing-extensions<5.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (4.4.0)\nCollecting pydash<6.0.0\n  Downloading pydash-5.1.1-py3-none-any.whl (84 kB)\n\u001b[K     |████████████████████████████████| 84 kB 5.0 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: opencensus-ext-azure<2.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (1.1.7)\nRequirement already satisfied: tqdm<5.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (4.64.1)\nCollecting marshmallow<4.0.0,>=3.5\n  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n\u001b[K     |████████████████████████████████| 49 kB 6.6 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (1.3.2)\nCollecting strictyaml<2.0.0\n  Downloading strictyaml-1.6.2.tar.gz (130 kB)\n\u001b[K     |████████████████████████████████| 130 kB 100.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: msrest>=0.6.18 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-ai-ml) (0.7.1)\nRequirement already satisfied: cryptography>=2.1.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-storage-blob<13.0.0,>=12.10.0->azure-ai-ml) (38.0.1)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (0.18.1)\nRequirement already satisfied: attrs>=17.4.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (22.1.0)\nRequirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (5.10.0)\nRequirement already satisfied: pkgutil-resolve-name>=1.3.10; python_version < \"3.9\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (1.3.10)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azure-ai-ml) (1.16.0)\nRequirement already satisfied: requests>=2.18.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-core!=1.22.0,<2.0.0,>=1.8.0->azure-ai-ml) (2.28.1)\nRequirement already satisfied: azure-identity<2.0.0,>=1.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (1.7.0)\nRequirement already satisfied: opencensus<1.0.0,>=0.11.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (0.11.0)\nRequirement already satisfied: psutil>=5.6.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus-ext-azure<2.0.0->azure-ai-ml) (5.9.3)\nRequirement already satisfied: packaging>=17.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.5->azure-ai-ml) (21.3)\nRequirement already satisfied: python-dateutil>=2.6.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from strictyaml<2.0.0->azure-ai-ml) (2.8.2)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest>=0.6.18->azure-ai-ml) (2022.9.24)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msrest>=0.6.18->azure-ai-ml) (1.3.1)\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.10.0->azure-ai-ml) (1.15.1)\nRequirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (3.9.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.18.4->azure-core!=1.22.0,<2.0.0,>=1.8.0->azure-ai-ml) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.18.4->azure-core!=1.22.0,<2.0.0,>=1.8.0->azure-ai-ml) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests>=2.18.4->azure-core!=1.22.0,<2.0.0,>=1.8.0->azure-ai-ml) (1.26.12)\nRequirement already satisfied: msal<2.0.0,>=1.12.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (1.20.0)\nRequirement already satisfied: msal-extensions~=0.3.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.3.1)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.10.2)\nRequirement already satisfied: opencensus-context>=0.1.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.5->azure-ai-ml) (3.0.9)\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml) (3.2.2)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.10.0->azure-ai-ml) (2.21)\nRequirement already satisfied: portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from msal-extensions~=0.3.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.6.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (1.56.4)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (2.13.0)\nCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n  Downloading protobuf-4.21.9-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n\u001b[K     |████████████████████████████████| 408 kB 78.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.2.8)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (5.2.0)\nRequirement already satisfied: pyasn1>=0.1.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0; python_version >= \"3.6\"->opencensus<1.0.0,>=0.11.0->opencensus-ext-azure<2.0.0->azure-ai-ml) (0.4.8)\nBuilding wheels for collected packages: strictyaml\n  Building wheel for strictyaml (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25h  Created wheel for strictyaml: filename=strictyaml-1.6.2-py3-none-any.whl size=123923 sha256=de48f0923cf727c26140516b3795d4a48beac7e1018af1d57493aaa9034b5579\n  Stored in directory: /home/azureuser/.cache/pip/wheels/e9/0b/fc/5beda6bad2ff803e820e157845679794a18f83e442da1e9f4d\nSuccessfully built strictyaml\n\u001b[31mERROR: tensorflow 2.2.1 has requirement h5py<2.11.0,>=2.10.0, but you'll have h5py 3.7.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorflow 2.2.1 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.21.6 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorflow-gpu 2.2.1 has requirement h5py<2.11.0,>=2.10.0, but you'll have h5py 3.7.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorflow-gpu 2.2.1 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.21.6 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorflow-cpu 2.2.1 has requirement h5py<2.11.0,>=2.10.0, but you'll have h5py 3.7.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorflow-cpu 2.2.1 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.21.6 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorboardx 2.5.1 has requirement protobuf<=3.20.1,>=3.8.0, but you'll have protobuf 4.21.9 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorboard 2.2.2 has requirement google-auth<2,>=1.6.3, but you'll have google-auth 2.13.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: ray 2.0.0 has requirement click<=8.0.4,>=7.0, but you'll have click 8.1.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: ray 2.0.0 has requirement grpcio<=1.43.0,>=1.28.1; python_version < \"3.10\", but you'll have grpcio 1.50.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: ray 2.0.0 has requirement protobuf<4.0.0,>=3.15.3, but you'll have protobuf 4.21.9 which is incompatible.\u001b[0m\n\u001b[31mERROR: onnx 1.12.0 has requirement protobuf<=3.20.1,>=3.12.2, but you'll have protobuf 4.21.9 which is incompatible.\u001b[0m\n\u001b[31mERROR: azureml-automl-runtime 1.47.0 has requirement protobuf<=3.20.1, but you'll have protobuf 4.21.9 which is incompatible.\u001b[0m\n\u001b[31mERROR: autokeras 1.0.16 has requirement tensorflow<=2.5.0,>=2.3.0, but you'll have tensorflow 2.2.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: azure-storage-file-datalake 12.9.1 has requirement azure-storage-blob<13.0.0,>=12.14.1, but you'll have azure-storage-blob 12.13.0 which is incompatible.\u001b[0m\nInstalling collected packages: azure-storage-file-datalake, azure-storage-file-share, pydash, marshmallow, strictyaml, azure-ai-ml, protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.1\n    Uninstalling protobuf-3.20.1:\n      Successfully uninstalled protobuf-3.20.1\nSuccessfully installed azure-ai-ml-1.1.2 azure-storage-file-datalake-12.9.1 azure-storage-file-share-12.10.1 marshmallow-3.19.0 protobuf-4.21.9 pydash-5.1.1 strictyaml-1.6.2\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1669799850524
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a compute resource to run your job\n",
        "\n",
        "You'll need a compute resource for running a job. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n",
        "\n",
        "You'll provision a Linux compute cluster. See the [full list on VM sizes and prices](https://azure.microsoft.com/pricing/details/machine-learning/) .\n",
        "\n",
        "For this example, you only need a basic cluster, so you'll use a Standard_DS11_v2 model and create an Azure ML Compute."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster-flights\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_DS11_V2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=4,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    \n",
        "\n",
        "print(\n",
        "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Creating a new cpu compute target...\nAMLCompute with name cpu-cluster-flights is created, the compute size is STANDARD_DS11_V2\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1669799853786
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Create Environnment for our execution"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run your AzureML job on your compute resource, you'll need an environment. An environment lists the software runtime and libraries that you want installed on the compute where you’ll be training. It's similar to your python environment on your local machine.\n",
        "\n",
        "AzureML provides many curated or ready-made environments, which are useful for common training and inference scenarios. You can also create your own custom environments using a docker image, or a conda configuration.\n",
        "\n",
        "In this example, you'll create a custom conda environment for your jobs, using a conda yaml file.\n",
        "\n",
        "First, create a directory to store the file in."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "env_dir = \"./env\"\n",
        "os.makedirs(env_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1669799861825
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create the Dockerfile where our pipeline will run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {env_dir}/Dockerfile\n",
        "FROM mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20220902.v1\n",
        "\n",
        "ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/responsibleai-0.21\n",
        "\n",
        "# Install wkhtmltopdf for pdf rendering from html\n",
        "RUN apt-get -y update && apt-get -y install wkhtmltopdf\n",
        "\n",
        "# Create conda environment\n",
        "RUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n",
        "    python=3.8 pip=21.3.1 -c anaconda -c conda-forge\n",
        "\n",
        "# Prepend path to AzureML conda environment\n",
        "ENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\n",
        "\n",
        "# Install pip dependencies\n",
        "# markupsafe and itsdangerous are bug workarounds\n",
        "RUN pip install 'responsibleai~=0.21.0' \\\n",
        "                'raiwidgets~=0.21.0' \\\n",
        "                'pyarrow' \\\n",
        "                'markupsafe<=2.0.1' \\\n",
        "                'itsdangerous==2.0.1' \\\n",
        "                'mlflow==1.30.0' \\\n",
        "                'scikit-learn<1.1' \\\n",
        "                'pdfkit==1.0.0' \\\n",
        "                'plotly==5.6.0' \\\n",
        "                'kaleido==0.2.1' \\\n",
        "                'azureml-core==1.45.0' \\\n",
        "                'azureml-dataset-runtime==1.45.0' \\\n",
        "                'azureml-mlflow==1.45.0' \\\n",
        "                'azureml-telemetry==1.45.0'\\\n",
        "                'seaborn'\\\n",
        "                'matplotlib'\\\n",
        "                'pyspark>=3.1,<3.2'\n",
        "\n",
        "RUN pip install --pre 'azure-ai-ml'\n",
        "\n",
        "# no-deps install for domonic due to unresolvable dependencies requirment on urllib3 and requests.\n",
        "# score card rendering is using domonic only for the html elements composer which does not involve requests or urllib3\n",
        "RUN pip install --no-deps 'charset-normalizer==2.0.12' \\\n",
        "                          'cssselect==1.1.0' \\\n",
        "                          'elementpath==2.5.0' \\\n",
        "                          'html5lib==1.1' \\\n",
        "                          'webencodings==0.5.1' \\\n",
        "                          'domonic==0.9.10'\n",
        "\n",
        "# This is needed for mpi to locate libpython\n",
        "ENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH/lib:$LD_LIBRARY_PATH\n",
        "\n",
        "# This is needed for pyspark to locate Java\n",
        "RUN apt-get update && \\\n",
        "    mkdir -p /usr/share/man/man1 && \\\n",
        "    apt-get install -y openjdk-8-jdk && \\\n",
        "    apt-get install -y ant && \\\n",
        "    apt-get clean && \\\n",
        "    rm -rf /var/lib/apt/lists/ && \\\n",
        "    rm -rf /var/cache/oracle-jdk8-installer;\n",
        "    \n",
        "ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\n",
        "RUN export JAVA_HOME\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./env/Dockerfile\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use a job to register our environment into our workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "from azure.ai.ml.entities import BuildContext\n",
        "\n",
        "custom_env_name = \"flight-delays-custom-env\"\n",
        "\n",
        "buildcontext = BuildContext(\n",
        "    path=env_dir\n",
        ")\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for spark flight delays\",\n",
        "    tags={\"owner\": \"romain.caret\", \"created\": \"2022-11-23\"},\n",
        "    build=buildcontext,\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading env (0.0 MBs):   0%|          | 0/2530 [00:00<?, ?it/s]\r\u001b[32mUploading env (0.0 MBs): 100%|██████████| 2530/2530 [00:00<00:00, 70476.12it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name flight-delays-custom-env is registered to workspace, the environment version is 9\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1669799887523
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Writing our component\n",
        "First of all, we are creating a yml file. This file will be a description of our azure ml component. It explains how this component works."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "component_dir = \"./components\"\n",
        "os.makedirs(component_dir, exist_ok=True)\n",
        "\n",
        "analysis_src_dir = \"./components/src\"\n",
        "os.makedirs(analysis_src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1669800336891
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $component_dir/flight_delays_analysis.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: analysis\n",
        "display_name: analysis\n",
        "version: 29\n",
        "type: command\n",
        "inputs:\n",
        "  public_data_flight_delays: \n",
        "    type: uri_file\n",
        "outputs:\n",
        "  eval_output:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:flight-delays-custom-env@latest\n",
        "command: >-\n",
        "  python analysis.py\n",
        "  --public_data_flight_delays ${{inputs.public_data_flight_delays}} \n",
        "  --eval_output ${{outputs.eval_output}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./components/flight_delays_analysis.yml\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1669192288076
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to actually create the code that will be executed by our component"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {analysis_src_dir}/analysis.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import datetime, warnings, scipy \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "from collections import OrderedDict\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pathlib import Path\n",
        "\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--public_data_flight_delays\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--eval_output\", type=str, help=\"path to eval output\")\n",
        "    args = parser.parse_args()\n",
        "   \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    ###################\n",
        "    #<load the data>\n",
        "    ###################\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.public_data_flight_delays)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    flights = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays, \"flights.csv\"), header=True)\n",
        "    airlines_names = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays,\"airlines.csv\"), header=True).toPandas()\n",
        "    \n",
        "    ####################\n",
        "    #</load the data>\n",
        "    ####################\n",
        "\n",
        "    ####################\n",
        "    #<Metrics rows and columns>\n",
        "    ####################\n",
        "    mlflow.log_metric(\"nb_rows\", flights.count())\n",
        "    mlflow.log_metric(\"nb_columns\", len(flights.columns))\n",
        "\n",
        "    ####################\n",
        "    #</Metrics rows and columns>\n",
        "    ####################\n",
        "\n",
        "    ####################\n",
        "    #<sample csv load>\n",
        "    ####################\n",
        "    tab_info=pd.DataFrame(flights.dtypes).T.rename(index={0:'column type'})\n",
        "\n",
        "    sample = flights.limit(10).toPandas()\n",
        "\n",
        "    sample.to_csv(\"sample.csv\")\n",
        "\n",
        "    mlflow.log_artifact(\"sample.csv\")\n",
        "\n",
        "    flights.createOrReplaceTempView(\"flights\")\n",
        "\n",
        "    airline_delays = spark.sql( \\\n",
        "        \"SELECT AIRLINE, MIN(ARRIVAL_DELAY) as min, MAX(ARRIVAL_DELAY) as max, AVG(ARRIVAL_DELAY) as avg, count(*) as count \\\n",
        "        from flights WHERE MONTH == 1 AND ARRIVAL_DELAY is not null \\\n",
        "        group by AIRLINE order by count\")\n",
        "\n",
        "    tab_pandas = airline_delays.toPandas()\n",
        "    \n",
        "    tab_pandas.to_csv(\"airline_delays.csv\")\n",
        "\n",
        "    mlflow.log_artifact(\"airline_delays.csv\")\n",
        "\n",
        "    ####################\n",
        "    #</sample csv load>\n",
        "    ####################\n",
        "\n",
        "\n",
        "    ####################\n",
        "    #<Graph Visualisation>\n",
        "    ####################\n",
        "    # 1.6 Flight & Mean Flight Delay distribution per Airline\n",
        "\n",
        "    abbr_companies = airlines_names.set_index('IATA_CODE')['AIRLINE'].to_dict()\n",
        "    global_stats = airline_delays.toPandas()\n",
        "    font = {'family' : 'normal', 'weight' : 'bold', 'size'   : 15}\n",
        "    mpl.rc('font', **font)\n",
        "    #__________________________________________________________________\n",
        "    colors = ['royalblue', 'grey', 'wheat', 'c', 'firebrick', 'seagreen', 'lightskyblue',\n",
        "            'lightcoral', 'yellowgreen', 'gold', 'tomato', 'violet', 'aquamarine', 'chartreuse']\n",
        "    #___________________________________\n",
        "    fig = plt.figure(1, figsize=(16,15))\n",
        "    gs=GridSpec(2,2)             \n",
        "    ax1=fig.add_subplot(gs[0,0]) \n",
        "    ax2=fig.add_subplot(gs[0,1])  \n",
        "    #------------------------------\n",
        "    # Pie chart nº1: nb of flights\n",
        "    #------------------------------\n",
        "    labels = [s for s in  global_stats.index]\n",
        "    sizes  = global_stats['count'].values\n",
        "    explode = [0.3 if sizes[i] < 20000 else 0.0 for i in range(len(abbr_companies))]\n",
        "    patches, texts, autotexts = ax1.pie(sizes, explode = explode,\n",
        "                                    labels=labels, colors = colors,  autopct='%1.0f%%',\n",
        "                                    shadow=False, startangle=0)\n",
        "    for i in range(len(abbr_companies)): \n",
        "        texts[i].set_fontsize(14)\n",
        "    ax1.axis('equal')\n",
        "    ax1.set_title('% of flights per company', bbox={'facecolor':'midnightblue', 'pad':5},\n",
        "                color = 'w',fontsize=18)\n",
        "    #_______________________________________________\n",
        "    # I set the legend: abreviation -> airline name\n",
        "    comp_handler = []\n",
        "    i = 0\n",
        "    for company in abbr_companies:\n",
        "        comp_handler.append(mpatches.Patch(color=colors[i],\n",
        "                label = abbr_companies[company]))\n",
        "        i = i + 1\n",
        "    ax1.legend(handles=comp_handler, bbox_to_anchor=(0.2, 0.9), \n",
        "            fontsize = 13, bbox_transform=plt.gcf().transFigure)\n",
        "    #----------------------------------------\n",
        "    # Pie chart nº2: mean delay at arrival\n",
        "    #----------------------------------------\n",
        "    sizes  = global_stats['avg'].values\n",
        "    sizes  = [max(s,0) for s in sizes]\n",
        "    explode = [0.0 if sizes[i] < 20000 else 0.01 for i in range(len(abbr_companies))]\n",
        "    patches, texts, autotexts = ax2.pie(sizes, explode = explode, labels = labels,\n",
        "                                    colors = colors, shadow=False, startangle=0,\n",
        "                                    autopct = lambda p :  '{:.0f}'.format(p * sum(sizes) / 100))\n",
        "    for i in range(len(abbr_companies)): \n",
        "        texts[i].set_fontsize(14)\n",
        "    ax2.axis('equal')\n",
        "    ax2.set_title('Mean delay at arrival', bbox={'facecolor':'midnightblue', 'pad':5},\n",
        "                color='w', fontsize=18)\n",
        "    #________________________\n",
        "    plt.tight_layout(w_pad=3)\n",
        "    plt.subplots()\n",
        "    #________________________\n",
        "    mlflow.log_figure(fig, \"flight_delay_per_airline.png\")\n",
        "\n",
        "    ####################\n",
        "    #</Graph Visualisation>\n",
        "    ####################\n",
        "\n",
        "    # 1.7. How airport impacts delays - Mean delays at arrival for each airport for Southwest airline\n",
        "\n",
        "    # Reset plot\n",
        "    plt.clf()\n",
        "\n",
        "    airline = \"'WN'\" # South West\n",
        "\n",
        "    airports_delays = spark.sql( \\\n",
        "    \"SELECT DESTINATION_AIRPORT, MIN(ARRIVAL_DELAY) as min, MAX(ARRIVAL_DELAY) as max, AVG(ARRIVAL_DELAY) as avg, count(*) as count \\\n",
        "    from flights WHERE MONTH == 1 AND AIRLINE == \" + airline + \" AND ARRIVAL_DELAY is not null \\\n",
        "    group by DESTINATION_AIRPORT order by DESTINATION_AIRPORT\")\n",
        "\n",
        "    airports_delays.limit(10).toPandas().to_csv(\"airports_delays.csv\")\n",
        "    mlflow.log_artifact(\"airports_delays.csv\")\n",
        "    \n",
        "\n",
        "    # Output of our component\n",
        "    eval_msg = f\"Eval done\\n\"\n",
        "    (Path(args.eval_output) / \"eval_result.txt\").write_text(eval_msg)\n",
        "   \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./components/src/analysis.py\n"
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Creating our component with a command\n",
        "Open a terminal in this folder\n",
        "\n",
        "**cd components**\n",
        "\n",
        "Now we must log into our azure account with the right directory\n",
        "Check your Directory ID here : https://portal.azure.com/#settings/directory\n",
        "\n",
        "Now login using your Directory ID\n",
        "\n",
        "**az login --tenant d3bc2180-cb1e-40f7-b59a-154105743342**\n",
        "\n",
        "After you successfuly login, set the right subscription ID you are currently using in Microsoft Azure Machine learning Studio.\n",
        "\n",
        "<img src=\"images/sub.PNG\" width=\"410\" height=\"360\" />\n",
        "\n",
        "**az account set --subscription 8b5374c5-8b98-45fb-bd96-7d5a4ce4e527**\n",
        "\n",
        "You are now perfectly set up and can create a component with this command using your workspace-name and ressource-group :\n",
        "\n",
        "**az ml component create --file flight_delays_analysis.yml --workspace-name romain-mlbox --resource-group rg-sbx-aiops**\n",
        "\n",
        "It should display a JSON with informations of the component you just created"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Creating a pipeline with the designer tool\n",
        "\n",
        "Open a new window of Azure ML Studio\n",
        "\n",
        "Go to Pipelines and create a new pipeline drafts\n",
        "\n",
        "Select Custom using custom components\n",
        "\n",
        "You should now see the designer tool\n",
        "\n",
        "Now select in Data, the dataset-delays-flights and drag it into the pipeline model\n",
        "\n",
        "Select Component and drop analysis component into the pipeline model\n",
        "\n",
        "Now drag the Data output port into public_flight_delays input port\n",
        "\n",
        "Rename the pipeline with the name : **Analysis Delay Flights**\n",
        "\n",
        "Go to settings and Select a Compute Cluster type and select **cpu-cluster-flights** that we create earlier\n",
        "\n",
        "Your screen should look like this :\n",
        "\n",
        "<img src=\"images/designer_tool.PNG\" width=\"800\" height=\"390\" />\n",
        "\n",
        "It is time to press Submit to launch this pipeline for the first time\n",
        "\n",
        "**Note that a .yml file can completely replace the design tool**\n",
        "\n",
        "In Assets Jobs on the left you should see your job\n",
        "\n",
        "Pipeline job overview is where you should see your pipeline running. It can take 5 to 15 minutes to run (installing environment on a node...)\n",
        "\n",
        "**Important**\n",
        "Whenever the job is complete, you can see a lot of informations in the component analysis that has finished :\n",
        "\n",
        "-Output + Logs = when you see the csv that has been generate by the pipeline and the output file\n",
        "\n",
        "-Metrics : Values that have been registered such as nb of rows\n",
        "* 5,819,079 rows\n",
        "* 31 columns\n",
        "\n",
        "-Images : Images that have been loaded during the execution"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7. How airport impacts delays - Mean delays at arrival for each airport for Southwest airline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Model training: Single airport LAX, Single airline WN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previsous sections dealt with an exploration of the dataset.  \n",
        "From here, we start with the modeling of flight delays. \n",
        "\n",
        "**Motivations**\n",
        "\n",
        "There is a high variability in average delays, both between the different airports but also between the different airlines. So, it is necessary to learn model that is specific to an airline the destination airport.\n",
        "\n",
        "We will learn model that predicts the flight delays at the destination airport at a given time of arrival (on January). We will work on Southwest Airlines (WN) flights arriving at the airport of Los Angeles International Airport (LAX). We will use the 3 first weeks of January as the training set and the follwoing week of January as the test set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Training Component\n",
        "\n",
        "Now we create our second component which will create a model and train the model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $component_dir/flight_delays_training.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: train_delay_model\n",
        "display_name: train_delay_model\n",
        "version: 8\n",
        "type: command\n",
        "inputs:\n",
        "  public_data_flight_delays: \n",
        "    type: uri_file\n",
        "  num_trees:\n",
        "    type: integer\n",
        "    default: 100\n",
        "outputs:\n",
        "  eval_output:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:flight-delays-custom-env@latest\n",
        "command: >-\n",
        "  python train.py\n",
        "  --public_data_flight_delays ${{inputs.public_data_flight_delays}} \n",
        "  --num_trees ${{inputs.num_trees}}\n",
        "  --eval_output ${{outputs.eval_output}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./components/flight_delays_training.yml\n"
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the code of our component"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {analysis_src_dir}/train.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import datetime, warnings, scipy \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "from collections import OrderedDict\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pathlib import Path\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "\n",
        "def format_date(year, month, day, scheduled):\n",
        "    if scheduled == 2400: \n",
        "        scheduled = 0\n",
        "    scheduled = \"{0:04d}\".format(int(scheduled))\n",
        "    return datetime.datetime(year, month, day, int(scheduled[0:2]), int(scheduled[2:4])).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def format_hour(scheduled):\n",
        "    if scheduled == 2400: \n",
        "        scheduled = 0\n",
        "    scheduled = \"{0:04d}\".format(int(scheduled))\n",
        "    return int(scheduled[0:2])\n",
        "    \n",
        "def format_seconds(scheduled):\n",
        "    if scheduled == 2400: \n",
        "        scheduled = 0\n",
        "    scheduled = \"{0:04d}\".format(int(scheduled))\n",
        "    return (3600 * int(scheduled[0:2])) + (60 * int(scheduled[2:4]))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--public_data_flight_delays\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--num_trees\", type=int, required=False, default=100)\n",
        "    parser.add_argument(\"--eval_output\", type=str, help=\"path to eval output\")\n",
        "    args = parser.parse_args()\n",
        "   \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    ###################\n",
        "    #<load the data>\n",
        "    ###################\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.public_data_flight_delays)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    flights = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays, \"flights.csv\"), header=True)\n",
        "    airlines_names = spark.read.option(\"inferschema\", \"true\").csv(os.path.join(args.public_data_flight_delays,\"airlines.csv\"), header=True).toPandas()\n",
        "    \n",
        "    ####################\n",
        "    #</load the data>\n",
        "    ####################\n",
        "    udf_format_date = F.udf(format_date, StringType())\n",
        "    udf_format_hour = F.udf(format_hour, IntegerType())\n",
        "    udf_format_seconds = F.udf(format_seconds, IntegerType())\n",
        "    flights = flights \\\n",
        "        .withColumn('SCHEDULED_DEPARTURE_FORMATTED',udf_format_date(flights.YEAR, flights.MONTH, flights.DAY, flights.SCHEDULED_DEPARTURE)) \\\n",
        "        .withColumn('SCHEDULED_ARRIVAL_FORMATTED',udf_format_date(flights.YEAR, flights.MONTH, flights.DAY, flights.SCHEDULED_ARRIVAL)) \\\n",
        "        .withColumn('SCHEDULED_ARRIVAL_hour',udf_format_hour(flights.SCHEDULED_ARRIVAL)) \\\n",
        "        .withColumn('SCHEDULED_ARRIVAL_sec',udf_format_seconds(flights.SCHEDULED_ARRIVAL))\n",
        "\n",
        "    flights.limit(10).toPandas().to_csv(\"flights.csv\")\n",
        "    mlflow.log_artifact(\"flights.csv\")\n",
        "\n",
        "    airline = \"'WN'\"\n",
        "    airport = \"'LAX'\"\n",
        "\n",
        "    flights.createOrReplaceTempView(\"flights\")\n",
        "\n",
        "    df_fi = spark.sql( \\\n",
        "        \"SELECT DAY_OF_WEEK, SCHEDULED_ARRIVAL_sec, DAY, ARRIVAL_DELAY as label \\\n",
        "        from flights WHERE AIRLINE == \" + airline + \" AND DESTINATION_AIRPORT == \" + airport +  \" AND ARRIVAL_DELAY is not null AND MONTH == 1\")\n",
        "\n",
        "    df_fi.limit(10).toPandas().to_csv(\"df_fi.csv\")\n",
        "    mlflow.log_artifact(\"df_fi.csv\")\n",
        "\n",
        "    df_train = df_fi.filter(F.col('DAY') < 23).drop('DAY')\n",
        "    df_test = df_fi.filter(F.col('DAY') > 23).drop('DAY')\n",
        "    print(\"training dataset size: \" + str(df_train.count()))\n",
        "    print(\"test dataset size: \" + str(df_test.count()))\n",
        "    df_train.limit(10).toPandas().to_csv(\"df_train.csv\")\n",
        "    mlflow.log_artifact(\"df_train.csv\")\n",
        "\n",
        "    assembler = VectorAssembler(\n",
        "    inputCols=[\"DAY_OF_WEEK\", \"SCHEDULED_ARRIVAL_sec\"], outputCol=\"features\"\n",
        "    )\n",
        "\n",
        "    X_train = assembler.transform(df_train)\n",
        "    X_train.limit(10).toPandas().to_csv(\"X_train.csv\")\n",
        "    mlflow.log_artifact(\"X_train.csv\")\n",
        "\n",
        "    # 2.2.1 Build, Train Model and visualize feature importances\n",
        "    # Train a RandomForest model\n",
        "    rf = RandomForestRegressor(numTrees=args.num_trees, featuresCol='features',labelCol='label',predictionCol='prediction')\n",
        "    rfModel = rf.fit(X_train)\n",
        "\n",
        "    importances = rfModel.featureImportances\n",
        "\n",
        "    x_values = list(range(len(importances)))\n",
        "\n",
        "    # Visualize the feature importances\n",
        "    plt.bar(x_values, importances, orientation = 'vertical')\n",
        "    plt.xticks(x_values, [\"DAY_OF_WEEK\", \"SCHEDULED_ARRIVAL_sec\"], rotation=40)\n",
        "    plt.ylabel('Importance')\n",
        "    plt.xlabel('Feature')\n",
        "    plt.title('Feature Importances')\n",
        "\n",
        "    plt.savefig(\"feature_importances.png\")\n",
        "    mlflow.log_artifact(\"feature_importances.png\")\n",
        "\n",
        "    # 2.3 Model evaluation\n",
        "    X_test = assembler.transform(df_test)\n",
        "    predictions = rfModel.transform(X_test)\n",
        "    predictions.select(\"prediction\", \"label\", \"features\").limit(10).toPandas().to_csv(\"predictions.csv\")\n",
        "    mlflow.log_artifact(\"predictions.csv\")\n",
        "\n",
        "    evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "    mse = evaluator.evaluate(predictions)\n",
        "    print(\"Mean Squared Error (MSE) on test data = %g\" % mse)\n",
        "\n",
        "    mlflow.log_metric(\"mse\", mse)\n",
        "\n",
        "    # 2.4 Model deployment\n",
        "    #TODO: add model deployment code\n",
        "\n",
        "    # Save the model\n",
        "    mlflow.spark.log_model(rfModel, \"model_delays_flight\")\n",
        "    mlflow.spark.save_model(rfModel, \"model_delays_flight\")\n",
        "\n",
        "\n",
        "\n",
        "    # Output of our component\n",
        "    eval_msg = f\"Eval done\\n\"\n",
        "    (Path(args.eval_output) / \"eval_result.txt\").write_text(eval_msg)\n",
        "   \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./components/src/train.py\n"
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Creating an other component with a command\n",
        "Open a terminal in this folder\n",
        "\n",
        "**cd components**\n",
        "\n",
        "Now we should already been logged. If there is any problem go to the login instructions\n",
        "\n",
        "**az ml component create --file flight_delays_training.yml --workspace-name romain-mlbox --resource-group rg-sbx-aiops**\n",
        "\n",
        "It should display a JSON with informations of the component you just created"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Updating our pipeline with the designer tool\n",
        "\n",
        "Open a new window of Azure ML Studio\n",
        "\n",
        "Go to Pipelines and create a new pipeline drafts\n",
        "\n",
        "Select Custom using custom components\n",
        "\n",
        "You should now see the designer tool\n",
        "\n",
        "Select Component and drop analysis component into the pipeline model\n",
        "\n",
        "Now drag the Data output port into public_flight_delays input port from train_delay_model component\n",
        "\n",
        "Rename the pipeline with the name : **Analysis and Train Delay Flights**\n",
        "\n",
        "Click on the train_delay_model component, you should be able to see a parameters called num_trees which is used by our component when it trains our model with randomForest algorithm\n",
        "\n",
        "\n",
        "It is time to press Submit to launch this pipeline\n",
        "\n",
        "Now you can see that analysis composent finished instantly because all the steps have been reused fron the precedent execution\n",
        "\n",
        "**Note that a .yml file can completely replace the design tool**\n",
        "\n",
        "**Important**\n",
        "Whenever the job is complete, you can see a lot of informations in the component analysis/train_delay_model that has finished :\n",
        "\n",
        "-Output + Logs = when you see the csv that has been generate by the pipeline and the output file\n",
        "\n",
        "-Metrics : Values that have been registered such as nb of rows\n",
        "* 5,819,079 rows\n",
        "* 31 columns\n",
        "\n",
        "-Images : Images that have been loaded during the execution"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}